Learning Spark:
--------------

##################################################################################################
#### Book: Learning Spark
#### Author: Holden Karau, Andy Konwinski, Patrick Wendell & Matei Zaharia
#### Available: Feb 2015
#### Spark Version Used: 1.1.0
#### 
#### IRead: Mar 2016 (after 1 year), Duration: xx Days took to finish
#### Spark Version Available: 1.6.1
#### 
#### JT Rating: x.x/5
#### Book Focused on: 
#### 
#### Chapters to Ignore: (if any)
##################################################################################################

############################MY-TRACKS#############################################################
#### CH_1_DONE => 3 hours (21-MAR-2016)
#### CH_2_DONE => 2 hours (21-MAR-2016)
#### CH_3_DONE => 5 hours (21-MAR-2016)
#### CH_4_DONE => 8 hours (22-MAR-2016)
#### CH_5_DONE => 8 hours (23-MAR-2016)

##################################################################################################

***************************************************************************************************
****************************************CHAPTER-ONE************************************************
***************************************************************************************************

CHP-1-"Introduction to Data Analysis with Spark":

What Is Apache Spark?
Ans:
	Apache Spark is a cluster computing platform designed to be fast and generalpurpose.
	
Why Is Apache Spark?
Ans:
	Spark extends the popular MapReduce model to efficiently support
		more types of computations, including interactive queries and stream processing.
	Spark runs on Memory as well as Disk.
	Spark is designed to cover a wide range of workloads that previously
		required separate distributed systems, including batch applications, iterative
		algorithms, interactive queries, and streaming.
	Spark is designed to be highly accessible, offering simple APIs in Python, Java, Scala,
		and SQL, and rich built-in libraries.
	Spark can run in Hadoop clusters and access any Hadoop data	source, including Cassandra.	

A Unified Stack: 
	Spark project contains multiple closely integrated components.

Spark Core:
	At its core, Spark
		is a “computational engine” that is responsible for scheduling, distributing, and monitoring
		applications consisting of many computational tasks across many worker
		machines, or a computing cluster.
	Spark Core is also home to the API that defines resilient distributed datasets
		(RDDs), which are Spark’s main programming abstraction.
		
Spark SQL:
	Spark SQL was added to Spark in version 1.0.
	Spark SQL supports many sources of data, including Hive tables, Parquet, and JSON.
	Spark SQL allows developers
		to intermix SQL queries with the programmatic data manipulations supported by
		RDDs in Python, Java, and Scala, all within a single application, thus combining SQL
		with complex analytics.
	
	Shark:
		Shark was an older SQL-on-Spark project out of the University of California, Berkeley,
			that modified Apache Hive to run on Spark. It has now been replaced by Spark
			SQL to provide better integration with the Spark engine and language APIs.
		
Spark Streaming:

	Spark Streaming is a Spark component that enables processing of live streams of data.	
	Spark Streaming provides an API for manipulating data streams that closely matches the Spark Core’s RDD API.
	Underneath its API, Spark Streaming was designed to provide the
		same degree of fault tolerance, throughput, and scalability as Spark Core.
	
MLlib:

	Spark comes with a library containing common machine learning (ML) functionality, called MLlib.
	MLlib provides multiple types of machine learning algorithms, including
		classification, 
		regression, 
		clustering, 
		and collaborative filtering, 
		as well as supporting functionality 
			such as model evaluation 
			and data import.
	All of these methods are designed to scale out across a cluster.
	
GraphX:

	GraphX is a library for manipulating graphs and performing graph-parallel computations.
	GraphX also provides various operators
		for manipulating graphs (e.g., subgraph and mapVertices) and a library of
		common graph algorithms (e.g., PageRank and triangle counting).
	
Cluster Managers:
	
	Spark is designed to efficiently scale up from one to many thousands of compute nodes.
	Spark can run over a variety of cluster managers, 
		including Hadoop YARN, Apache Mesos, and a simple cluster manager included 
		in Spark itself called the Standalone Scheduler.
		
Who Uses Spark, and for What?
	
Data Science Tasks:
	
	While there is no standard definition, for our purposes a data scientist
		is somebody whose main task is to analyze and model data. Data scientists may have
		experience with SQL, statistics, predictive modeling (machine learning), and programming,
		usually in Python, Matlab, or R.

	Data scientists use their skills to analyze data with the goal of answering a question or
		discovering insights.
		
	Spark enables data scientists to tackle problems with larger data sizes than they could before
		with tools like R or Pandas. There is support for calling out to external programs in Matlab or R

	For example, the initial investigation of a data scientist might lead to the creation of a 
		production recommender system that is integrated into a web application and used to generate product suggestions to users. 
	
	Often it is a different person or team that leads the process of productizing the work of the data scientists,
		and that person is often an engineer.

Data Processing Applications: All other things like, to integrate them into production systems.
	
History of Spark:
	
	Unk, 2009 - Started at UC Berkeley
	Mar, 2010 - Open Sourced
	Jun, 2013 - Transferred to Apache Software Foundation (ASF)
	
Storage Layers for Spark:
	
	Spark can create distributed datasets from any file stored in the Hadoop distributed filesystem (HDFS) 
		or other storage systems supported by the Hadoop APIs 
			(including your local filesystem, 
							Amazon S3, 
							Cassandra, 
							Hive, 
							HBase, etc.).
	It’s important to remember that Spark does not require Hadoop; 
		it simply has support for storage systems implementing the Hadoop APIs.

	Spark supports 
		text files, 
		SequenceFiles, 
		Avro, 
		Parquet, 
		and any other Hadoop InputFormat.

***************************************************************************************************
****************************************CHAPTER-TWO************************************************
***************************************************************************************************

CHP-2-"Downloading Spark and Getting Started":

	Spark can be used from Python, Java, or Scala.
	Spark itself is written in Scala, and runs on the Java Virtual Machine (JVM).
	Spark does not yet work with Python 3.
	
	This chapter will be with Spark running in local mode
	
	Spark can run in a variety of different modes, or environments. 
		Beyond local mode, Spark can also be run on 
		Mesos, 
		YARN, 
		or the Standalone Scheduler included in the Spark distribution.

	Unlike most other shells, however, which let you manipulate data using the disk and
		memory on a single machine, Spark’s shells allow you to interact with data that is distributed
		on disk or in memory across many machines, and Spark takes care of automatically
		distributing this processing.	
		
	Because Spark can load data into memory on the worker nodes, many distributed
		computations, even ones that process terabytes of data across dozens of machines,
		can run in a few seconds.

Start Python Shell:

	bin/pyspark
	
Start Scala Shell:

	bin/spark-shell
	
	You may find the logging statements that get printed in the shell distracting. 
	You can control the verbosity of the logging. To do this, you can create a file in the conf directory called log4j.properties. 
	The Spark developers already include a template for this file called log4j.properties.template.

	To make the logging less verbose, make a copy of
		conf/log4j.properties.template called conf/log4j.properties and find the following line:

		log4j.rootCategory=INFO, console
		
		convert to
		
		log4j.rootCategory=WARN, console

	Using IPython:
		
		IPython is an enhanced Python shell that many Python users prefer,
			offering features such as tab completion. You can find instructions
			for installing it at http://ipython.org.

		SET IPYTHON environment variable
			IPYTHON=1 ./bin/pyspark
			
		To use the IPython Notebook, which is a web-browser-based version of IPython, use:
			IPYTHON_OPTS="notebook" ./bin/pyspark
			
RDD: Resilient Distributed Datasets

	In Spark, we express our computation through operations on distributed collections
		that are automatically parallelized across the cluster. 
	These collections are called resilient distributed datasets, or RDDs. 
	RDDs are Spark’s fundamental abstraction for distributed data and computation.
			
	http://[ipaddress]:4040. 
		You can access the Spark UI there and see all sorts of information about your tasks and cluster.

Introduction to Core Spark Concepts:

	At a high level, 
		every Spark application consists of a driver program that launches various parallel operations on a cluster.

	The driver program contains your application’s main function 
		and defines distributed datasets on the cluster, then applies operations to them.
		
	Here, the driver program was the Spark shell itself, 
		and you could just type in the operations you wanted to run.
		
	Driver programs access Spark through a SparkContext object, 
		which represents a connection to a computing cluster. 
		In the shell, a SparkContext is automatically created for you as the variable called "sc".
		
	Once you have a SparkContext, you can use it to build RDDs.

	To run these operations, driver programs typically manage a number of nodes called "executors".
		
	Finally, a lot of Spark’s API revolves around passing functions to its operators to run them on the cluster	
		
	While we will cover the Spark API in more detail later, 
		a lot of its magic is that function-based operations like filter also parallelize across the cluster.
		
	That is, Spark automatically takes your function (e.g., line.contains("Python")) 
		and ships it to executor nodes. 
	Thus, you can write code in a single driver program and automatically have parts of it run on multiple nodes.
		
	"Passing Functions to Spark"	: more details on other notes of mine.
		If you are unfamiliar with the lambda or => syntax, 
			it is a shorthand way to define functions inline in Python and Scala.
		When using Spark in these languages, 
			you can also define a function separately and then pass its name to Spark.
		Passing functions to Spark is also possible in Java, 
			but in this case they are defined as classes, implementing an interface called Function.

		Java 8 introduces shorthand syntax called lambdas that looks similar to Python and Scala.

Standalone Applications: (not interactive shells, code in files, ".java" like)

	Apart from running interactively, Spark can be linked into standalone applications
		in either Java, Scala, or Python.
	The main difference from using it in the shell is that you need to initialize your own SparkContext. 
		After that, the API is the same.

	Eclipse allow you to directly add a Maven dependency to a project.
	
	In Python, you simply write applications as Python scripts, but you must run them
		using the "bin/spark-submit" script included in Spark.
		
	The spark-submit script includes the Spark dependencies for us in Python.

	Once you have linked an application to Spark, you need to import the Spark packages in your program and create a SparkContext. 
		You do so by first creating a "SparkConf" object to configure your application, and then building a "SparkContext" for it.

	Example 2-8. Initializing Spark in Scala
		import org.apache.spark.SparkConf
		import org.apache.spark.SparkContext
		import org.apache.spark.SparkContext._

		val conf = new SparkConf().setMaster("local").setAppName("My App")
		val sc = new SparkContext(conf)

	A "cluster URL", namely "local" in these examples, which tells Spark how to connect to a cluster. 
		"local" is a special value that runs Spark on one thread on the local machine, without connecting to a cluster.

	An "application name", namely "My App" in these examples. 
		This will identify your application on the cluster manager’s UI if you connect to a cluster.

	Additional parameters exist for configuring how your application executes or adding code to be shipped to the cluster
	After you have initialized a SparkContext, 
		you can use all the methods we showed before to create RDDs (e.g., from a text file) and manipulate them.
	Finally, to shut down Spark, you can either call the stop() method on your SparkContext, 
		or simply exit the application (e.g., with System.exit(0) or sys.exit()).

Build:

	We’ve marked the Spark Core dependency as provided so that, 
		later on, when we use an assembly JAR we don’t include the spark-core JAR, which is already on the classpath of the workers.

	<project>
		<groupId>com.oreilly.learningsparkexamples.mini</groupId>
		<artifactId>learning-spark-mini-example</artifactId>
		<modelVersion>4.0.0</modelVersion>
		<name>example</name>
		<packaging>jar</packaging>
		<version>0.0.1</version>
		<dependencies>
			<dependency> <!-- Spark dependency -->
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-core_2.10</artifactId>
			<version>1.2.0</version>
			<scope>provided</scope>
			</dependency>
		</dependencies>
		<properties>
			<java.version>1.6</java.version>
		</properties>
		<build>
			<pluginManagement>
			<plugins>
				<plugin>
					<groupId>org.apache.maven.plugins</groupId>
					<artifactId>maven-compiler-plugin</artifactId>
					<version>3.1</version>
					<configuration>
						<source>${java.version}</source>
						<target>${java.version}</target>
					</configuration> 
				</plugin>
			</plugins>
			</pluginManagement>
		</build>
	</project>

	The spark-core package is marked as "provided" in case we package our application into an assembly JAR.

	$ mvn clean && mvn compile && mvn package
	
Run:
	
	$ $SPARK_HOME/bin/spark-submit \
	--class com.oreilly.learningsparkexamples.mini.java.WordCount \
	./target/learning-spark-mini-example-0.0.1.jar \
	./README.md ./wordcounts

	A driver program 
		creates a SparkContext and RDDs, 
		and then runs parallel operations on them

***************************************************************************************************
**************************************CHAPTER-THREE************************************************
***************************************************************************************************

CHP-3-"Programming with RDDs":

	An RDD is simply a distributed collection of elements.
	
	In Spark all work is expressed as either 
		(1) creating new RDDs, 
		(2) or transforming existing RDDs, 
		(3) or calling operations on RDDs to compute a result.

	Under the hood, Spark automatically 
		(1) distributes the data contained in RDDs across your cluster 
		(2) and parallelizes the operations you perform on them.

	An RDD in Spark is simply an "immutable" distributed collection of objects.
		
	Each RDD is 
		split into multiple "partitions", 
		which may be computed on different nodes of the cluster. 
	RDDs can contain 
		any type of Python, Java, or Scala objects, 
		including userdefined classes.
		
	Users create RDDs in two ways: 
		(1) by loading an external dataset,  [	val abc = SparkContext.textFile(/abc.txt)	]
		(2) or by distributing a collection of objects (e.g., a list or set) in their driver program. 
			[	val lines = sc.parallelize(List("pandas", "i like pandas"))	], not widely used, as it need's entire data in memory.
		
	Once created, 
		RDDs offer two types of operations: 
			(1) transformations 
			(2) and actions.
		
		Transformations construct a new RDD from a previous one. [i.e. it returns another RDD]
			For example, one common transformation is filtering data that matches a literal string(AKA predicate).
		
		Actions, on the other hand, compute a result based on an RDD, 
			and either return it to the driver program 
			or save it to an external storage system (e.g., HDFS).
			One example of an action we called earlier is "first()", which returns the first element in an RDD
		
	Transformations and actions are different 
		because of the way Spark computes RDDs.
	Although you can define new RDDs any time, Spark computes them only in a lazy fashion, 
		that is, the first time they are used in an action.
	This approach might seem unusual at first, 
		but makes a lot of sense when you are working with Big Data.	
		
	If Spark were to load and store all the lines in the file as soon as we wrote lines = sc.textFile(...), 
		it would waste a lot of storage space, given that we then immediately filter out many lines. 
	Instead, once Spark sees the whole chain of transformations, it can compute just the data needed for its result. 
	
	In fact, for the first() action, 
		Spark scans the file only until it finds the first matching line; it doesn’t even read the whole file.	[OPTIMIZATION]
		
	Finally, Spark’s RDDs are by default recomputed each time you run an action on them.
		If you would like to reuse an RDD in multiple actions, you can ask Spark to persist it using RDD.persist().

	After computing it the first time,
		Spark will store the RDD contents in memory (partitioned across the machines in your cluster), 
		and reuse them in future actions.
	Persisting RDDs on disk instead of memory is also possible.

	The behavior of not persisting by default may again seem unusual, 
		but it makes a lot of sense for big datasets: 
		if you will not reuse the RDD,there’s no reason to waste storage space 
		when Spark could instead stream through the data once and just compute the result.

	To summarize, every Spark program and shell session will work as follows:
		1. Create some input RDDs from external data.
		2. Transform them to define new RDDs using transformations like filter().
		3. Ask Spark to persist() any intermediate RDDs that will need to be reused.
		4. Launch actions such as count() and first() to kick off a parallel computation,
			which is then optimized and executed by Spark.

	cache() is the same as calling persist() with the "default storage level".

RDD Operations:
	
	Transformations are operations on RDDs that return a new RDD, such as map() and filter().

	Actions are operations that 
		return a result to the driver program 
		or write it to storage, 
		and kick off a computation, such as count() and first().

	If you are ever confused whether a given function is a transformation or an action, 
		you can look at its return type: transformations return RDDs, whereas actions return some other data type.

Transformations:

	Transformations are operations on RDDs that return a new RDD. 
	As discussed in “Lazy Evaluation”, 
		transformed RDDs are computed lazily, only when you use them in an action. 
	Many transformations are element-wise; that is, they work on one element at a time; 
		but this is not true for all transformations.
	
	Example 3-12. filter() transformation in Scala
		val inputRDD = sc.textFile("log.txt")
		val errorsRDD = inputRDD.filter(line => line.contains("error"))
			
	Note that the filter() operation does not mutate the existing "inputRDD". 
		Instead, it returns a pointer to an entirely new RDD.

	Transformations can actually operate on any number of input RDDs.
		for example, union() is a bit different than filter(), in that it operates on two RDDs instead of one.

	Finally, as you derive new RDDs from each other using transformations,
		Spark keeps track of the set of dependencies between different RDDs, 
		called the "lineage graph".

	It uses this information to compute each RDD on demand 
		and to recover lost data if part of a persistent RDD is lost.

Actions:

	Actions are the second type of RDD operation.
	Actions force the evaluation of the transformations required for the RDD 
		they were called on, since they need to actually produce output.

	Example 3-16. Scala error count using actions
		println("Input had " + badLinesRDD.count() + " concerning lines")
		println("Here are 10 examples:")
		badLinesRDD.take(10).foreach(println)

	We used take() to retrieve a small number of elements in the RDD at the driver program. 
		We then iterate over them locally to print out information at the driver.
	
	RDDs also have a collect() function to retrieve the entire RDD.
		This can be useful if your program filters RDDs down to a very small size and you’d like to deal with it locally.
	
	Keep in mind that your entire dataset must fit in memory on a single machine to use collect() on it, 
		so collect() shouldn’t be used on large datasets.

	In most cases RDDs can’t just be collect()ed to the driver because they are too large. 
	In these cases, it’s common to write data out to a 
		distributed storage system 
			such as HDFS 
			or Amazon S3.
	You can save the contents of an RDD using the saveAsTextFile() action, saveAsSequenceFile(), 
		or any of a number of actions for various built-in formats.

	It is important to note that each time we call a new action, the entire RDD must be computed “from scratch.” 
	To avoid this inefficiency, users can persist intermediate results.
	
Lazy Evaluation:
	
	As you read earlier, 
		TRANSFORMATIONS on RDDs are lazily evaluated, 
		meaning that,
		Spark will not begin to execute until it sees an ACTION.

	Lazy evaluation means that when we call a transformation on an RDD (for instance, calling map()), 
		the operation is not immediately performed. 
		Instead, Spark internally records metadata to indicate that this operation has been requested.

	Rather than thinking of an RDD 
		as "containing specific data", 
			it is best to think of each RDD as
		"consisting of instructions" on how to compute the data that we build up through transformations.

	Loading data into an RDD is lazily evaluated in the same way transformations are. 
	So, when we call sc.textFile(), the data is not loaded until it is necessary.

	Although transformations are lazy, you can force Spark to execute them at any time by running an action, such as count(). 
	This is an easy way to test out just part of your program.
	
	Spark uses lazy evaluation to reduce the number of passes it has to take over our data
		by grouping operations together.
		
	In systems like Hadoop MapReduce, developers often have to spend a lot of time 
		considering how to group together operations to minimize the number of MapReduce passes.
	In Spark, there is no substantial benefit to writing a single complex map instead of chaining together many simple operations.
	
	Thus, users are free to organize their program into smaller, more manageable operations.

Passing Functions to Spark:

	Most of Spark’s transformations, and some of its actions, depend on passing in functions
		that are used by Spark to compute data.

	Types of "Passing Functions"
	
		Python:
		(1) pass lambda expressions
		(2) pass in top-level functions
		(3) pass locally defined functions
		
		One issue to watch out for when passing functions is inadvertently serializing the object containing the function.
		When you pass a function that is the member of an object, or contains references to fields in an object (e.g., self.field), 
			Spark sends the entire object to worker nodes, which can be much larger than the bit of information you need.
		
		Sometimes this can also cause your program to fail, 
			if your class contains objects that Python can’t figure out how to pickle.
		
		Scala:
		(1) pass in functions defined inline
		(2) pass references to methods
		(3) pass static functions
		
		The function we pass and the data referenced in it needs to be serializable (implementing Java’s Serializable interface).

		If NotSerializableException occurs in Scala, a reference to a method or field in a nonserializable class 
			is usually the problem. Note that passing in local serializable variables 
			or functions that are members of a top-level object is always safe.
		
		Java:
		In Java, functions are specified as objects that 
			implement one of Spark’s function interfaces from the org.apache.spark.api.java.function package.
		
		There are a number of different interfaces based on the return type of the function.
		
		TABLE of COMMON Functions:
		
			Function name				Method to implement				Example
			-------------				-------------------				-------
			Function<T, R>				R call(T)						map() or filter()
			Function2<T1, T2, R>		R call(T1, T2)					aggregate() or fold()
			FlatMapFunction<T, R>		Iterable<R> call(T)				flatMap()
		
		We can either define our function classes 
			(1) inline as anonymous inner classes or
			(2) or create a named class
			
		Example 3-22. Java function passing with anonymous inner class
			RDD<String> errors = lines.filter(new Function<String, Boolean>() {
				public Boolean call(String x) { return x.contains("error"); }
			});

		Example 3-23. Java function passing with named class
			class ContainsError implements Function<String, Boolean>() {
				public Boolean call(String x) { return x.contains("error"); }
			}
			
			RDD<String> errors = lines.filter(new ContainsError());
		
		Benefits of TOP LEVEL named functions:
			(1) are often cleaner for organizing large programs
			(2) can give them constructor parameters [in other words, can avoid hard coding like above]
		
		Example 3-24. Java function class with parameters
			class Contains implements Function<String, Boolean>() {
				private String query;
				public Contains(String query) { this.query = query; }
				public Boolean call(String x) { return x.contains(query); }
			}
				
			RDD<String> errors = lines.filter(new Contains("error"));		
		
		Using LAMBDA expressions, JAVA 8, more simple:
		
			RDD<String> errors = lines.filter(s -> s.contains("error"));
		
Common Transformations and Actions:

	Basic RDDs:
	
		Element-wise transformations:
		
		map() 
			transformation takes in a function and "applies it to each element in the RDD" 
				with the result of the function being the new value of each element in the resulting RDD.
		
		filter() 
			transformation takes in a function and returns an RDD 
				that only has elements that pass the filter() function.
		
		Change RDD type by using map().
		We can use map() to do any number of things, from fetching the website associated
			with each URL in our collection to just squaring the numbers.
		It is useful to note that map()’s return type does not have to be the same as its input type, 
			so if we had an RDD String and our map() function were to parse the strings and return a Double,
			our input RDD type would be RDD[String] and the resulting RDD type would be RDD[Double].

		flatMap()
			Sometimes we want to produce multiple output elements for each input element.[in map() one output for one input]
			Instead of returning a single element, we return an iterator with our return values.
			Rather than producing an RDD of iterators, we get back an RDD that consists of the elements
				from all of the iterators.

		Example 3-31. flatMap() in Java, splitting lines into multiple words
			JavaRDD<String> lines = sc.parallelize(Arrays.asList("hello world", "hi"));
			JavaRDD<String> words = lines.flatMap(new FlatMapFunction<String, String>() {
				public Iterable<String> call(String line) {
					return Arrays.asList(line.split(" "));
				}
			});
			words.first(); // returns "hello"
		
		Think of flatMap() as “flattening” the iterators returned to it, 
			so that instead of ending up with an RDD as "collection of small collections" 
			we will simply have a "big collection".
		
		Example with words:
		
		RDD 			=> {"coffee panda", "happy panda", "happiest panda party"}
		mappedRDD 		=> {["coffee", "panda"], ["happy", "panda"], ["happiest", "panda", "party"]}
		flatMappedRDD 	=> {"coffee", "panda", "happy", "panda", "happiest", "panda", "party"}

		Pseudo set operations:
		
			RDDs support many of the operations of mathematical sets, such as union and intersection,
				even when the RDDs themselves are not properly sets.
			All of these operations require that the RDDs being operated on are of the same type.

			RDD1.distinct()
			RDD1.union(RDD2)
			RDD1.intersection(RDD2)
			RDD1.subtract(RDD2)
			RDD1.cartesian(RDD2)

			If we want only unique elements we can use the RDD.distinct() transformation 
				to produce a new RDD with only distinct items. distinct() is COSTLY.
				
			Spark’s union() will contain duplicates.
			Spark’s intersection() also removes all duplicates.
			
			intersection(), subtract(), distinct() uses SHUFFLE operation on network, its costly.

	Actions:
	
	reduce()
	With reduce(), we can easily sum the elements of our RDD, 
		count the number of elements, and perform other types of aggregations

	fold()
	similar to reduce(), in addition takes a “zero value” to be used for the initial call on each partition.
	The zero value you provide should be the identity element for your operation; 
		that is, applying it multiple times with your function should not change the value 
		(e.g., 0 for +, 1 for *, or an empty list for concatenation).

	Both fold() and reduce() require that the return type of our result be the same type
		as that of the elements in the RDD we are operating over.

	aggregate()
		function frees us from the constraint of having the return be the same type as the RDD we are working on.

	Example 3-37. aggregate() in Java
		class AvgCount implements Serializable {
			public AvgCount(int total, int num) {
			this.total = total;
			this.num = num;
			}
			public int total;
			public int num;
			public double avg() {
				return total / (double) num;
			}
		}
		Function2<AvgCount, Integer, AvgCount> addAndCount =
			new Function2<AvgCount, Integer, AvgCount>() {
			public AvgCount call(AvgCount a, Integer x) {
			a.total += x;
			a.num += 1;
			return a;
			}
		};
		Function2<AvgCount, AvgCount, AvgCount> combine =
			new Function2<AvgCount, AvgCount, AvgCount>() {
			public AvgCount call(AvgCount a, AvgCount b) {
			a.total += b.total;
			a.num += b.num;
			return a;
			}
		};

		AvgCount initial = new AvgCount(0, 0);
		AvgCount result = rdd.aggregate(initial, addAndCount, combine);
		System.out.println(result.avg());

		IN OTHER WORDS: 
			initial, addAndCount, combine 	=> first-element(0,0), next-element, aggregate-of-both-elements
											=> previous-element, next-element, aggregate-of-both-elements


		The simplest and most common operation that returns data to our driver program is collect(), 
			which returns the entire RDD’s contents. Problem is data need to fit in memory.
		And, take(n) returns n elements from the RDD and attempts to minimize the number of
			partitions it accesses, so it may represent a biased collection.
		It’s important to note that these operations do not return the elements in the order you might expect.
		These operations are useful for unit tests and quick debugging, but may introduce
			bottlenecks when you’re dealing with large amounts of data.
			
		top()
			will use the default ordering on the data, 
				but we can supply our own comparison function to extract the top elements.
		
		takeSample(withReplacement, num, seed) 
			function allows us to take a sample of our data either with or without replacement.

		foreach()
			an action without returning anything.
			Sometimes it is useful to perform an action on all of the elements in the RDD, 
				but without returning any result to the driver program. 
			A good example of this would be posting JSON to a webserver or inserting records into a database.
	
		count()
			returns a count of the elements
			
		countByValue() 
			returns a map of each unique value to its count

Converting Between RDD Types:
	
	Some functions are available only on certain types of RDDs,
		such as mean() and variance() on numeric RDDs
		or join() on key/value pair RDDs.
		
	In Scala and Java, these methods aren’t defined on the standard RDD class, 
		so to access this additional functionality we have to make sure we get the correct specialized class.
	
	SCALA:
	In Scala the conversion to RDDs with special functions is handled automatically using implicit conversions.
	These implicits turn an RDD into various wrapper classes, such as DoubleRDDFunctions (for RDDs of numeric data) 
		and PairRDDFunctions (for key/value pairs), to expose additional functions such as mean() and variance().
	
	Implicits, while quite powerful, can sometimes be confusing. 
	If you call a function like mean() on an RDD, you might look at the Scaladocs for the RDD class and notice
		there is no mean() function. 
	The call manages to succeed because of implicit conversions between RDD[Double] and DoubleRDDFunctions. 
	When searching for functions on your RDD in Scaladoc, 
		make sure to look at functions that are available in these wrapper classes.
	
	JAVA:
	In Java the conversion between the specialized types of RDDs is a bit more explicit.
	In particular, there are special classes called JavaDoubleRDD and JavaPairRDD for RDDs of these types, 	
		with extra methods for these types of data. we can understand easily.
	
	To construct RDDs of these special types, instead of always using the Function class
		we will need to use specialized versions.
	
	If we want to create a DoubleRDD from an RDD of type T, 
		rather than using "Function<T, Double>" we use "DoubleFunction<T>".
	We also need to call different functions on our RDD 
		(so we can’t just create a DoubleFunction and pass it to map()).
	When we want a DoubleRDD back, instead of calling map(), we need to call mapToDouble() 
		with the same pattern all of the other functions
	
	PYTHON:
	The Python API is structured differently than Java and Scala. In Python all of the functions are implemented 
		on the base RDD class but will fail at runtime if the type of data in the RDD is incorrect.
	
Persistence (Caching):
	
	When we ask Spark to persist an RDD, the nodes that compute the RDD store their partitions.
	If a node that has data persisted on it fails, Spark will recompute the lost partitions of the data when needed.
	We can also replicate our data on multiple nodes if we want to be able to handle node failure without slowdown.

	Recomputing an entire RDD is especially expensive for iterative algorithms, which look at the data many times.
		A trivial example would be doing a count and then writing out the same RDD.

	Example 3-39. Double execution in Scala
		val result = input.map(x => x*x)
		println(result.count())
		println(result.collect().mkString(","))

	Spark has many levels of persistence to choose from based on what our goals are:
		In Scala and Java,
			the default persist() will store the data in the JVM heap as unserialized objects.
		In Python,
			we always serialize the data that persist stores, 
				so the default is instead stored in the JVM heap as pickled(serialized) objects.
			When we write data out to disk or off-heap storage, that data is also always serialized.

	[[Off-heap caching is experimental and uses Tachyon. If you are
	interested in off-heap caching with Spark, take a look at the Running
	Spark on Tachyon guide.]]

	The persist() call on its own doesn’t force evaluation.
	
	If you attempt to cache too much data to fit in memory, Spark will automatically
		evict(spill) old partitions using a Least Recently Used (LRU) cache policy.
	For the memoryonly storage levels, 
		it will recompute these partitions the next time they are accessed.
	For the memory-and-disk ones, it will write them out to disk.

	In either case, this means that you don’t have to worry about your job breaking if you ask Spark to cache too much data.
	However, caching unnecessary data can lead to eviction of useful data and more recomputation time.
	
***************************************************************************************************
**************************************CHAPTER-FOUR*************************************************
***************************************************************************************************

CHP-4-"Working with Key/Value Pairs":

	Key/Value pairs are common data type required for many operations in Spark.
	They are used to perform aggregations, or some new operations
		and often we will do some initial ETL to get our data into a key/value format.
	Using controllable partitioning, applications can sometimes greatly reduce communication costs 
		by ensuring that data will be accessed together and will be on the same node.

	Choosing the right partitioning for a distributed dataset is similar 
		to choosing the right data structure for a local one—in both cases, data layout can greatly affect performance.

	Spark provides special operations on RDDs containing key/value pairs. 
	These RDDs are called "pair RDDs".
	
	pair RDDs have,
		reduceByKey(), method that can aggregate data separately for each key
		join(), method that can merge two RDDs together by grouping elements with the same key
	
	It is common to extract fields from an RDD and use those fields as keys in pair RDD operations.

Creating Pair RDDs:

	There are a number of ways to get pair RDDs in Spark.
	Many input formats at loading time will directly return pair RDDs for their key/value data.
	We can turn a "regular RDD" into "pair RDD". [by using map()]
	
	The way to build key-value RDDs differs by language.
	
	Python:
	The functions on keyed data to work we need to return an RDD composed of tuples
	pairs = lines.map(lambda x: (x.split(" ")[0], x))

	Scala:
	The functions on keyed data to work we also need to return an RDD composed of tuples
	val pairs = lines.map(x => (x.split(" ")(0), x))
	
	Java:
	Java doesn’t have a built-in tuple type,
		so Spark’s Java API helps users to create tuples using the scala.Tuple2 class.
	Java users can construct a new tuple by writing "new Tuple2(elem1, elem2)"
	And can then access its elements with the "._1()" and "._2()" methods.

	Java users also need to call special versions of Spark’s functions when creating pairRDDs.
	For instance, 
		the mapToPair() function should be used in place of the basic map() function.

	PairFunction<String, String, String> keyData =
		new PairFunction<String, String, String>() {
		public Tuple2<String, String> call(String x) {
			return new Tuple2(x.split(" ")[0], x);
		}
	};
	JavaPairRDD<String, String> pairs = lines.mapToPair(keyData);

	When creating a pair RDD from an in-memory collection,
		In Scala and Python,
			we need to call "SparkContext.parallelize()"
		In Java,
			we need to call "SparkContext.parallelizePairs()"

Transformations on Pair RDDs:

	Pair RDDs are allowed to use all the transformations available to standard RDDs.
	Since pair RDDs contain tuples, 
		we need to pass functions that "operate on tuples" rather than on individual elements.

	Table 4-1. Transformations on one pair RDD
	Table 4-2. Transformations on two pair RDD
	
	Pair RDDs are also still RDDs (of Tuple2 objects in Java/Scala or of Python tuples),
		and thus support the same functions as RDDs.

	Example 4-6. Simple filter on second element in Java
		Function<Tuple2<String, String>, Boolean> longWordFilter =
			new Function<Tuple2<String, String>, Boolean>() {
				public Boolean call(Tuple2<String, String> keyValue) {
					return (keyValue._2().length() < 20);
			}
			};
		JavaPairRDD<String, String> result = pairs.filter(longWordFilter);

Aggregations:
	When datasets are described in terms of key/value pairs, it is common to want to
		aggregate statistics across all elements with the same key.
	These operations return RDDs and thus are transformations rather than actions.

	reduceByKey() is quite similar to reduce(); both take a function and use it to combine values.
	reduceByKey() runs several parallel reduce operations, one for each key in the dataset, 
		where each operation combines values that have the same key.

	foldByKey() is quite similar to fold();both use a zero value of the same type of the
		data in our RDD and combination function.
	
	Those familiar with the combiner concept from MapReduce should note that calling reduceByKey() and foldByKey() will
		automatically perform combining locally on each machine before computing global totals for each key. 
	The user does not need to specify a combiner. The more general combineByKey() interface 
		allows you to customize combining behavior
	
	WordCount in one line,
		val newRDD = input.flatMap(x => x.split(" ")).countByValue()

	combineByKey() is the most general of the per-key aggregation functions.
	Like aggregate(), combineBy Key() allows the user to return values that are not the same type as our input data.
	As combineByKey() goes through the elements in a partition, each element either has a key it hasn’t seen before 
		or has the same key as a previous element.

	//combiner is like a datastructure in spark, understand more from the below URL
		http://abshinn.github.io/python/apache-spark/2014/10/11/using-combinebykey-in-apache-spark/
		http://stackoverflow.com/questions/29246756/how-createcombiner-mergevalue-mergecombiner-works-in-combinebykey-in-spark-us
		
	combineByKey(),
		Need to provide 3 functions as args, every computation happens on VALUE part of RDD
			First, to set initial value, called as "createCombiner()"
			Second, to increment/accumulate the subsequent value, called as "mergeValue()"
			Third, to accumulate/merge the values/accumulators across partitions, called as "mergeCombiners()"
		Since each partition is processed independently, we can have multiple accumulators for the same key.
		
		Example 4-14. Per-key average using combineByKey() in Java [see this later]
		In any case, using one of the specialized aggregation functions in Spark can be much faster than the
			naive approach of grouping our data and then reducing it.

Tuning the level of parallelism:

How Spark decides how to split up the work?
Ans:
	Every RDD has a fixed number of partitions 
		that determine the degree of parallelism to use when executing operations on the RDD.
	When performing aggregations or grouping operations, we can ask Spark to use a specific number of partitions.
	Spark will always try to infer a sensible default value based on the size of your cluster, 
		but in some cases you will want to tune the level of parallelism for better performance.

How to tell the number of partitions to use explicitly?
Ans:
	By using a second parameter.
	Example 4-15. reduceByKey() with custom parallelism in Python
		data = [("a", 3), ("b", 4), ("a", 1)]
		sc.parallelize(data).reduceByKey(lambda x, y: x + y) # Default parallelism
		sc.parallelize(data).reduceByKey(lambda x, y: x + y, 10) # Custom parallelism

	repartition(),
		Which shuffles the data across the network to create a new set of partitions.
		Sometimes, we want to change the partitioning of an RDD outside the context of grouping and aggregation operations.
		It is a fairly expensive operation.
		
	coalesce(),
		optimized version of "repartition()"
		Used, to bring down the number of partitions, like many to few
		That allows avoiding data movement, but only if you are decreasing the number of RDD partitions.

How to verifiy partitions count?
Ans:
	In Java/Scala,
		rdd.partitions.size()
		
	In Python,
		rdd.getNumPartitions()

Grouping Data:

	groupByKey(),
		RDD consisting of keys of type K and values of type V, we get back an RDD of type [K, Iterable[V]].
		
	groupBy(),
		works on unpaired data
		or data where we want to use a "different condition" besides equality on the current key.
		It takes a function that it applies to every element in the source RDD and uses the result to determine the key.
		
	rdd.reduceByKey(func),
		produces same result as "rdd.groupByKey().mapValues(value => value.reduce(func))"
		but is more efficient as it "avoids the step of creating a list of values" for each key.

	cogroup(),
		we can group data sharing the same key from multiple RDDs.
		If two RDDs sharing the same key type, K, with the respective value types V and W 
			gives us back RDD[(K, (Iterable[V], Iterable[W]))]
		If one of the RDDs doesn’t have elements for a given key that is present in the other RDD, 
			the corresponding Iterable is simply empty
		Additionally, cogroup() can work on three or more RDDs at once.

Joins:
	
	join(),
		an inner join, usage: RDD1.join(RDD2)
		
	leftOuterJoin(other)
	rightOuterJoin(other)
	
	For Outer Joins,
		The value associated with each key in the result is a tuple of the value from the source RDD 
			and an Option (or Optional in Java) for the value from the other pair RDD.
		In Python, if a value isn’t present None is used; 
			and if the value is present the regular value, without any wrapper, is used.

		[[Optional is part of Google’s Guava library and represents a possibly missing value. 
			We can check isPresent() to see if it’s set, 
			and get() will return the contained instance provided data is present.]]

Sorting Data:

	sortByKey(),
		We can sort an RDD with key/value pairs provided that there is an ordering defined on the key
		Once we have sorted our data, any subsequent call on the sorted data 
			to collect() or save() will result in ordered data.
		Sometimes we want a different sort order entirely, 
			and to support this we can provide our own comparison function.
		Default is ascending order.
	
	Example 4-20. Custom sort order in Scala, "sorting integers as if strings"
		val input: RDD[(Int, Venue)] = ...
			implicit val sortIntegersByString = new Ordering[Int] {
			override def compare(a: Int, b: Int) = a.toString.compare(b.toString)
		}
		rdd.sortByKey()

Actions Available on Pair RDDs:

	As with the transformations, all of the traditional actions available on the base RDD
		are also available on pair RDDs.
		
	Additional Actions,
		countByKey(), Count the number of elements for each key.(a wordcount kind of result)
		collectAsMap(), Collect the result as a map to provide easy lookup. (like dictionary, lookup table)
		lookup(key), Return all values associated with the provided key.

Data Partitioning (Advanced):
	
	partitionBy(),
		a TRANSFORMATION, to Control datasets partitioning across nodes.
		that means,
			it always returns a new RDD
			it does not change the original RDD
	
	In a distributed program, communication is very expensive,
		so laying out data to minimize network traffic can greatly improve performance.
	Partitioning is useful only when a dataset is reused multiple times in key-oriented operations such as joins.
	If a given RDD is scanned only once, there is no point in partitioning it in advance.
	
	Spark’s partitioning is available on all RDDs of key/value pairs, 
		and causes the system to group elements based on a function of each key.
	Although Spark does not give explicit control of which worker node each key goes to (to support fault tolarence), 
		it lets the program ensure that a set of keys will appear together on some node.
	For example, 
		you might choose to hashpartition an RDD into 100 partitions 
			so that keys that have the same hash value modulo 100 appear on the same node. 
		Or you might range-partition the RDD into sorted ranges of keys 
			so that elements with keys in the same range appear on the same node.
	Explore, Example 4-22 later as usecase is good.
	
	Example 4-23. Scala custom partitioner
		val sc = new SparkContext(...)
		val userData = sc.sequenceFile[UserID, UserInfo]("hdfs://...")
			.partitionBy(new HashPartitioner(100)) // Create 100 partitions
			.persist()
	
	Because we called partitionBy() when building userData, Spark will now know that it is hash-partitioned, 
		and calls to join() on it will take advantage of this information.
	The result is that a lot less data is communicated over the network, and the program runs significantly faster. 
	
	And you make sure that the NUM-OF-PARTITIONS at least as large as the number of cores in your cluster.
	Without persistence, use of the partitioned RDD will cause reevaluation of the RDDs complete lineage. 
		That would negate the advantage of partitionBy(), 
		resulting in repeated partitioning and shuffling of data across the network

	Many other Spark operations automatically result in an RDD with known partitioning information
	sortByKey(), 
		result in range-partitioned
	groupByKey()
		result in hash-partitioned
	map(),
		ignores parent RDD partition information, as the key changes, so no use

	In Python, you cannot pass a Hash Partitioner object to partitionBy;
		instead, you just pass the number of partitions desired (e.g., rdd.partitionBy(100)).

Determining an RDD’s Partitioner:
	
	spark.Partitioner object,
		The "partitioner" property is a great way to test in the Spark shell how different
			Spark operations affect partitioning.
	
	Example 4-24. Determining partitioner of an RDD, shows how to use partitionsBy() and partitioner property
		scala> val pairs = sc.parallelize(List((1, 1), (2, 2), (3, 3)))
		pairs: spark.RDD[(Int, Int)] = ParallelCollectionRDD[0] at parallelize at <console>:12

		scala> pairs.partitioner
		res0: Option[spark.Partitioner] = None

		scala> val partitioned = pairs.partitionBy(new spark.HashPartitioner(2))
		partitioned: spark.RDD[(Int, Int)] = ShuffledRDD[1] at partitionBy at <console>:14

		scala> partitioned.partitioner
		res1: Option[spark.Partitioner] = Some(spark.HashPartitioner@5147788d)
	
	If we actually wanted to use partitioned in further operations, 
		then we should have appended persist() to the third line of input, in which partitioned is defined.
	Without persist(), subsequent RDD actions will "evaluate the entire lineage" of partitioned, 
		which will cause pairs to be hash-partitioned "over and over".
	
Operations That Benefit from Partitioning:
	
	Save SHUFFLING, which is costly.
	
	Many of Spark’s operations involve shuffling data by key across the network.
	For operations that act on a single RDD, such as reduceByKey(), 
		running on a prepartitioned RDD will cause all the values for each key to be computed locally on a single machine, 
		requiring only the final, locally reduced value to be sent from each worker node back to the master.
	Finally, for binary operations, 
		which partitioner is set on the output depends on the parent RDDs’ partitioners. 
	By default, it is a hash partitioner, with the number of partitions set to the level of parallelism of the operation.
	
Example: PageRank

	The PageRank algorithm, named after Google’s Larry Page,
		aims to assign a measure of importance (a “rank”) to each document in a set 
		based on how many documents have links to it.
	
	PageRank is an iterative algorithm that performs many joins, so it is a good use case for RDD partitioning.
	
	// i need to spend more time to understand the PAGE RANK example in detail
	
Custom Partitioners:	
	
	Spark also allows you to tune how an RDD is partitioned by providing a custom Partitioner object.
	This can help you further reduce communication by taking advantage of domain-specific knowledge.
	
	Make a custom partitioner for PAGE RANK.
	As we know that web pages within the same domain tend to link to each other a lot.
		(e.g., http://www.cnn.com/WORLD and http://www.cnn.com/US)
	or else they will be in different partitions as HASH is different for them.
	
	To implement a custom partitioner, you need to subclass 
		the org.apache.spark.Partitioner class and implement three methods:
		(1) numPartitions: Int, which returns the number of partitions you will create
		(2) getPartition(key: Any): Int, which returns the partition ID (like R0 to Rn-1 in hadoop)
		(3) equals(),
			This is important to implement because Spark will need to test your Partitioner object against other instances
				of itself when it decides whether two of your RDDs are partitioned the same way!
		gotcha,
			Java’s hashCode() method returns negative integer values also.
			make sure you override it to get always positive integers.
			
		[[Note that the hash function you pass will be compared by identity to that of other
			RDDs. If you want to partition multiple RDDs with the same partitioner, pass the
			same function object (e.g., a global function) instead of creating a new lambda for
			each one!]]

***************************************************************************************************
****************************************CHAPTER-FIVE***********************************************
***************************************************************************************************

CHP-5-"Loading and Saving Your Data":
	
	Engineers may wish to explore more output formats to see if there is something well suited to
		their intended downstream consumer.
	Data scientists can likely focus on the format that their data is already in.

	So far our examples have loaded and saved all of their data from
		(1) a native collection and 
		(2) regular files
	but odds are that your data doesn’t fit on a single machine.
	Therefore, need other options.
	
	Spark can access data through the "InputFormat" and "OutputFormat" interfaces used by Hadoop MapReduce, 
		which are available for many common file formats and storage systems. (s3, HDFS, HBase, etc.)

	we will cover three common sets of data sources:
		(1) File formats and filesystems
		(2) Structured data sources through Spark SQL
		(3) Databases and key/value stores
		
	File Formats:
		Formats range 
			from unstructured, like text, 
			to semistructured, like JSON, 
			to structured, like SequenceFiles

		Examples:
			(1) Text
			(2) JSON
			(3) CSV
			(4) SequenceFiles, A common Hadoop file format used for key/value data.
			(5) ProtocolBuffers, A fast, space-efficient multilanguage format.
			(6) ObjectFiles, Useful for saving data from a Spark job to be consumed by shared code, used Java Serialization.

	Also, in Spark, we can use both Hadoop’s new and old file APIs for keyed (or paired) data.
	We can use these only with key/value data, because the Hadoop interfaces require key/value data,

		Text Files:
			When we load a single text file as an RDD, each input line becomes an element in the RDD.
			We can also load multiple whole text files at the same time into a pair RDD, 
				with the key being the name and the value being the contents of each file.
			"SparkContext.wholeTextFiles()" method and get back a pair RDD where the key is
				the name of the input file.
			
			Example 5-2. Loading a text file in Scala
				val input = sc.textFile("file:///home/holden/repos/spark/README.md")

			Example 5-4. Average value per file in Scala [folder as input]
				val input = sc.wholeTextFiles("file://home/holden/salesFiles")
				val result = input.mapValues{y =>
						val nums = y.split(" ").map(x => x.toDouble)
				nums.sum / nums.size.toDouble
				}
			Spark supports reading all the files in a given directory and doing
				wildcard expansion on the input (e.g., part-*.txt).
			
			result.saveAsTextFile(outputFile), 
				to save an RDD into text file
				The path is treated as a directory and Spark will output multiple files underneath that directory.

		JSON Files:
		
			JSON is a popular semistructured data format.
			The simplest way to load JSON data is by loading the data as a text file
				and then mapping over the values with a JSON parser.
			We can use our preferred JSON serialization library to write out the values to strings,

			Loading JSON:
				This works assuming that you have one JSON record per row; 
				if you have multiline JSON files, you will instead have to load
					the whole file and then parse each file.
	
				If constructing a JSON parser is expensive in your language, you can use mapPartitions() to reuse the parser;
			Java and Scala will use "Jackson" to read/parse JSON files.
			In Scala and Java, it is common to load records into a class representing their schemas.
			
			Example 5-6. Loading unstructured JSON in Python
				import json
				data = input.map(lambda x: json.loads(x))
	
			In Scala and Java, it is common to load records into a class representing their schemas.
			Handling incorrectly formatted records can be a big problem, especially with semistructured data like JSON.
			For large data sets, issues are common with malformed JSON data, be prepared.
			
			Example 5-8. Loading JSON in Java
				class ParseJson implements FlatMapFunction<Iterator<String>, Person> {
					public Iterable<Person> call(Iterator<String> lines) throws Exception {
						ArrayList<Person> people = new ArrayList<Person>();
						ObjectMapper mapper = new ObjectMapper();
						while (lines.hasNext()) {
							String line = lines.next();
							try {
								people.add(mapper.readValue(line, Person.class));
							} catch (Exception e) {
								// skip records on failure
							}
						}
						return people;
					}
				}
				JavaRDD<String> input = sc.textFile("file.json");
				JavaRDD<Person> result = input.mapPartitions(new ParseJson());	
			
			Take our RDD of structured data and convert it into an RDD of strings, 
				which we can then write out using Spark’s text file API.
			
			Let’s say we were running a promotion for people who love pandas. We can take our
				input from the first step and filter it for the people who love pandas.
			
			Example 5-10. Saving JSON in Scala
				result.filter(p => P.lovesPandas).map(mapper.writeValueAsString(_))
						.saveAsTextFile(outputFile)
	
		CSV Files:(TSV also)
		
		Records are often stored one per line, but this is not always the case as records can sometimes span lines.
		CSVs cannot handle nested field types natively, so we have to unpack and pack to specific fields manually.
		
		Unlike with JSON fields, each record doesn’t have field names associated with it;
			instead we get back row numbers. 
		It is common practice in single CSV files to make the first row’s column values the names of each field.
		
		Loading CSV/TSV data is similar to loading JSON data in that 
			we can first load it as text and then process it.
			
		Python has built-in libraries for JSON/CSV/TSV.
		
		Example 5-13. Loading CSV with textFile() in Scala [if data happens to not contain newlines in any of the fields]
			import Java.io.StringReader
			import au.com.bytecode.opencsv.CSVReader
			...
			val input = sc.textFile(inputFile)
			val result = input.map{ line =>
					val reader = new CSVReader(new StringReader(line));
			reader.readNext();
			}
	
		If there are embedded newlines in fields, we will need to load each file in full and parse the entire segment.
			This is unfortunate because if each file is large it can introduce bottlenecks in loading and parsing.

		Example 5-17. Loading CSV in full in Java
			public static class ParseLine
				implements FlatMapFunction<Tuple2<String, String>, String[]> {
				public Iterable<String[]> call(Tuple2<String, String> file) throws Exception {
					CSVReader reader = new CSVReader(new StringReader(file._2()));
					return reader.readAll();
				}
			}
			JavaPairRDD<String, String> csvData = sc.wholeTextFiles(inputFile);
			JavaRDD<String[]> keyedRDD = csvData.flatMap(new ParseLine());		


		As with JSON data, writing out CSV/TSV data is quite simple and we can benefit from reusing the output encoding object.
		In Python, if we are outputting dictionaries the CSV writer can do this for us 
			based on the order in which we provide the fieldnames when constructing the writer.
		The CSV libraries we are using output to files/writers so we can use StringWriter/
			StringIO to allow us to put the result in our RDD.
		Repartition your input to allow Spark to effectively parallelize your future operations.
		
		pandaLovers.map(person => List(person.name, person.favoriteAnimal).toArray)
			.mapPartitions{people =>
				val stringWriter = new StringWriter();
				val csvWriter = new CSVWriter(stringWriter);
				csvWriter.writeAll(people.toList)
				Iterator(stringWriter.toString)
			}.saveAsTextFile(outFile)

		SequenceFiles:
		
			SequenceFiles are a popular Hadoop format composed of flat files with key/value pairs.
			SequenceFiles have sync markers that allow Spark to seek to a point in the file 
				and then resynchronize with the record boundaries. Spark can use them efficiently.
			Used widely with Hadoop too.
			SequenceFiles consist of elements that implement Hadoop’s Writable interface, 
				as Hadoop uses a custom serialization framework.
			SequenceFiles work with Writable classes, so our keyClass and valueClass 
				will both have to be the correct Writable class.
			**Hadoop’s RecordReader reuses the same object for each record, 
				so directly calling cache on an RDD you read in like this can fail;
				instead, add a simple map() operation and cache its result.

			In Spark 1.0 and earlier, SequenceFiles were available only in Java and Scala,
				but Spark 1.1 added the ability to load and save them in Python as well.
			The Python Spark API knows only how to convert the basic Writables available in Hadoop to Python
			
			Loading SequenceFiles:
			
			sequenceFile(path, keyClass, valueClass, minPartitions),
				
			In Scala there is a convenience function that can automatically convert Writables to their corresponding Scala type. 
				Instead of specifying the keyClass and valueClass, we can call sequence File[Key, Value](path, minPartitions) 
				and get back an RDD of native Scala types.
	
			Example 5-22. Loading a SequenceFile in Java
				public static class ConvertToNativeTypes implements
					PairFunction<Tuple2<Text, IntWritable>, String, Integer> {
						public Tuple2<String, Integer> call(Tuple2<Text, IntWritable> record) {
							return new Tuple2(record._1.toString(), record._2.get());
					}
				}
				JavaPairRDD<Text, IntWritable> input = sc.sequenceFile(fileName, Text.class, IntWritable.class);
				JavaPairRDD<String, Integer> result = input.mapToPair(new ConvertToNativeTypes());
	
			Saving SequenceFiles:
			
			Writing the data out to a SequenceFile is fairly similar in Scala.
			First, because SequenceFiles are key/value pairs, 
				we need a PairRDD with types that our SequenceFile can write out.
			Implicit conversions between Scala types and Hadoop Writables exist for many native types
			Just save your "PairRDD" by calling "saveAsSequenceFile(path)"
	
			Example 5-23. Saving a SequenceFile in Scala
				val data = sc.parallelize(List(("Panda", 3), ("Kay", 6), ("Snail", 2)))
				data.saveAsSequenceFile(outputFile)
	
		Object Files:
			Object files are a deceptively simple wrapper around SequenceFiles that allows us 
				to save our RDDs containing just values. 
			Unlike with SequenceFiles, with object files the values are written out using Java Serialization.
			If you change your classes—for example, to add and remove fields old object files may no longer be readable.
		
			Using Java Serialization for object files has a number of implications.
			Unlike with normal SequenceFiles, the output will be different than Hadoop outputting the same objects.
			Java Serialization can also be quite slow.
			
			objectFile(),
				to read an object file.
			saveAsObjectFile(),
				to save an object file from an RDD.

			The primary reason to use object files is that they require almost no work to save almost arbitrary objects.

			Object files are not available in Python, but the Python RDDs and SparkContext support
				methods called saveAsPickleFile() and pickleFile() instead.

		Hadoop Input and Output Formats:
			
			We can also interact with any Hadoop-supported formats. 
			Spark supports both the “old” and “new” Hadoop file APIs.

			To read in a file using the new Hadoop API we need to tell Spark a few things
				"newAPIHadoopFile" takes a path, and three classes.
					(1) format class
					(2) key class
					(3) value class
						(4) conf object [optional]
			Example 5-24. Loading KeyValueTextInputFormat() with old-style API in Scala
				val input = sc.hadoopFile[Text, Text, KeyValueTextInputFormat](inputFile).map{
						case (x, y) => (x.toString, y.toString)	
						}

			we can also load JSON data using a custom Hadoop input format.
				Twitter’s Elephant Bird package supports a large number of data formats, 
					including JSON, Lucene, Protocol Buffer–related formats, and others.

				Example 5-25. Loading LZO-compressed JSON with Elephant Bird in Scala
					val input = sc.newAPIHadoopFile(inputFile, classOf[LzoJsonInputFormat],
														classOf[LongWritable], classOf[MapWritable], conf)
					// Each MapWritable in "input" represents a JSON object
					// LZO support requires you to install the hadoop-lzo package and point Spark to its native libraries.
	
			In Java we don’t have the same convenience function for saving "SequenceFile" from a pair RDD.

			Example 5-26. Saving a SequenceFile in Java, using Hadoop Input/Output Format mechanisms
				public static class ConvertToWritableTypes implements
					PairFunction<Tuple2<String, Integer>, Text, IntWritable> {
						public Tuple2<Text, IntWritable> call(Tuple2<String, Integer> record) {
							return new Tuple2(new Text(record._1), new IntWritable(record._2));
						}
					}
				JavaPairRDD<String, Integer> rdd = sc.parallelizePairs(input);
				JavaPairRDD<Text, IntWritable> result = rdd.mapToPair(new ConvertToWritableTypes());
				result.saveAsHadoopFile(fileName, Text.class, IntWritable.class, SequenceFileOutputFormat.class);			

		Non-filesystem data sources:
			
			To access Hadoop-supported storage formats that are not filesystems. Like HBase, MongoDB, etc.
			Use Hadoop input formats to read directly from the key/value store.
			
			hadoopDataset(),
				family of functions just take a Configuration object on which you 
					set the Hadoop properties needed to access your data source.

			Protocol buffers,
				First developed at Google for internal remote procedure calls(RPC) later open sourced.
				Are structured data, with the fields and types of fields being clearly defined.
				Optimized to be fast for encoding and decoding.
				Take up the minimum amount of space.
				Compared to XML, PBs are 3× to 10× smaller and can be 20× to 100× faster to encode and decode.
				
				Not platform independent, need a special compiler for different language.
				Not self-describing, to parse it, you need to know the definition before hand.
				// go here for more, https://developers.google.com/protocol-buffers/
				// see examples 5-27 and 5-28 for further understanding.
				
		File Compression:
		
			Compression options apply only to the Hadoop formats that support compression,
				namely those that are written out to a filesystem.
			Database Hadoop formats generally do not implement support for compression.
			Spark’s textFile() method can handle compressed input.
			
			Compression options:
			
			Type			Splittable			Speed			Compression
			-----			----------			--------		------------
			gzip			N					Fast			High
			lzo				Y					Very Fast		Medium
			bzip2			Y					Slow			Very High
			zlib			N					Slow			Medium
			snappy			N					Very Fast		Low

			**Many of the formats in Twitter’s Elephant Bird package work with "LZO" compressed data.
		
	Filesystems:
		
		Local/“Regular” FS:
			While Spark supports loading files from the local filesystem, 
				it requires that the files are available at the same path on all nodes in your cluster.
	
			Some network filesystems, like NFS, AFS, and MapR’s NFS layer, are exposed to the user as a regular filesystem.
				access it by specifying file://
				
				Example 5-29. Loading a compressed text file from the local filesystem in Scala
					val rdd = sc.textFile("file:///home/holden/happypandas.gz")
				
			If your file isn’t already on all nodes in the cluster, you can load it locally on the driver 
				without going through Spark and then call parallelize to distribute the contents to workers.
			This approach can be slow, however, 
				so we recommend putting your files in a shared filesystem like HDFS, NFS, or S3.
		
		Amazon S3:
			To access S3 in Spark, you should first set the AWS_ACCESS_KEY_ID and
				AWS_SECRET_ACCESS_KEY environment variables to your S3 credentials.
			If you get an S3 access permissions error from Amazon, make sure that the account 
				for which you specified an access key has both “read” and “list” permissions on the bucket. 
			Spark needs to be able to list the objects in the bucket to identify the ones you want to read.
			Spark supports wildcard paths for S3, such as s3n://bucket/my-files/*.txt.

		HDFS:
			Spark and HDFS can be collocated on the same machines, 
				and Spark can take advantage of this data locality to avoid network overhead.
			Using Spark with HDFS is as simple as specifying hdfs://master:port/path for your input and output.

			The HDFS protocol changes across Hadoop versions,
				so if you run a version of Spark that is compiled for a different version it will fail.

	Structured Data with Spark SQL:
		
		Spark SQL is a component added in Spark 1.0 good for structured and semistructured data.
		Structured means schema is there.
		In all cases, we give Spark SQL a SQL query to run on the data source,
			and we get back an RDD of Row objects, one per record.
		
		Apache Hive:
			Spark SQL can load any table supported by Hive.
			Hive can store tables in a variety of formats, from plain text to column-oriented formats.

			To connect Spark SQL to an existing Hive installation, you need to provide a Hive configuration. 
			You do so by copying your hive-site.xml file to Spark’s ./conf/ directory.
			Once you have done this, you create a HiveContext object, which is the entry point to Spark SQL, 
				and you can write Hive Query Language (HQL) queries against your tables to get data back as RDDs of rows.


			Example 5-32. Creating a HiveContext and selecting data in Java

				import org.apache.spark.sql.hive.HiveContext;
				import org.apache.spark.sql.Row;
				import org.apache.spark.sql.SchemaRDD;

				HiveContext hiveCtx = new HiveContext(sc);
				SchemaRDD rows = hiveCtx.sql("SELECT name, age FROM users");
				Row firstRow = rows.first();
				System.out.println(firstRow.getString(0)); // Field 0 is the name
		
		JSON:
			If you have JSON data with a consistent schema across records, Spark SQL can infer their schema 
				and load this data as rows as well, making it very simple to pull out the fields you need.

			Example 5-33. Sample tweets in JSON [one tweet per line]
				{"user": {"name": "Holden", "location": "San Francisco"}, "text": "Nice day out today"}
				{"user": {"name": "Matei", "location": "Berkeley"}, "text": "Even nicer here :)"}

			Example 5-36. JSON loading with Spark SQL in Java
				SchemaRDD tweets = hiveCtx.jsonFile(jsonFile);
				tweets.registerTempTable("tweets");
				SchemaRDD results = hiveCtx.sql("SELECT user.name, text FROM tweets");

	Databases:
		
		Java Database Connectivity: 
			see, Example 5-37. "JdbcRDD" in Scala

		As with other data sources, when using JdbcRDD, make sure that your database can
			handle the load of parallel reads from Spark.

		Cassandra:
			Spark’s Cassandra support has improved greatly with the introduction of the open source 
				"Spark Cassandra connector" from DataStax.
			Cassandra doesn’t yet use Spark SQL, but it returns RDDs of CassandraRow objects, 
				which have some of the same methods as Spark SQL’s Row object.
			Cassandra connector reads a job property to determine which cluster to connect to.
			In addition to loading an entire table, we can also query subsets of our data.
			
			see, examples for further understanding.

		HBase:
			Spark can access HBase through its Hadoop input format.
			see, example for further understanding.
			
		Elasticsearch:
			Spark can both read and write data from Elasticsearch using "Elasticsearch-Hadoop".
			Elasticsearch is a new open source, Lucene-based search system.
			
			No Automatic, we need to set more properties by hand.

***************************************************************************************************
****************************************CHAPTER-SIX************************************************
***************************************************************************************************

CHP-6-"Advanced Spark Programming":

***************************************************************************************************
****************************************CHAPTER-SEVEN**********************************************
***************************************************************************************************

CHP-7-"Running on a Cluster":

	One benefit of writing applications on Spark is the ability to scale computation by adding more machines and running in cluster mode.

Spark Runtime Architecture:

	In distributed mode, Spark uses a master/slave architecture with one central coordinator and many distributed workers. 
	The central coordinator is called the driver. 
	The driver communicates with a potentially large number of distributed workers called executors. 
	The driver runs in its own Java process and each executor is a separate Java process. 
	A driver and its executors are together termed a Spark application.
	
	A Spark application is launched on a set of machines using an external service called a cluster manager. 
	As noted, Spark is packaged with a built-in cluster manager called the Standalone cluster manager. 
	Spark also works with Hadoop YARN and Apache Mesos, two popular open source cluster managers.

	The Driver:
	
		The driver is the process where the main() method of your program runs.
		It is the process running the user code that creates a SparkContext, creates RDDs, and performs transformations and actions.
		Once the driver terminates, the application is finished.
			When you launch a Spark shell, you’ve created a driver program.
		
		When the driver runs, it performs two duties:
			(1) Converting a user program into tasks
			(2) Scheduling tasks on executors
			
		Converting a user program into tasks:
			A Spark program implicitly creates a logical directed acyclic graph (DAG) of operations.
			When the driver runs, it converts this logical graph into a physical execution plan.
			Spark performs several optimizations, such as “pipelining” map transformations
				together to merge them, and converts the execution graph into a set of stages.
			Each stage, in turn, consists of multiple tasks. 
			The tasks are bundled up and prepared to be sent to the cluster. 
			Tasks are the smallest unit of work in Spark
			A typical user program can launch hundreds or thousands of individual tasks.
			
		Scheduling tasks on executors:
			Given a physical execution plan, 
				a Spark driver must coordinate the scheduling of individual tasks on executors.
			When executors are started they register themselves with the driver, 
				so it has a complete view of the application’s executors at all times.
			Each executor represents a process capable of running tasks and storing RDD data.
			
			The Spark driver will look at the current set of executors and try to schedule each
				task in an appropriate location, based on data placement
			The driver also tracks the location of cached data and uses it to schedule future tasks that access that data.
			
			The driver exposes information about the running Spark application through a
				web interface, which by default is available at port 4040.
					http://localhost:4040
					
	The Executor:
	
		Spark executors are worker processes responsible for running the individual tasks in a given Spark job.
		Executors are launched once at the beginning of a Spark application. Spark applications can continue if executors fail.
		
		Executors have two roles.
			First, they run the tasks that make up the application and return results to the driver.
			Second, they provide "in-memory storage" for RDDs that are cached by user programs, 
				through a service called the "Block Manager" that lives within each executor.
		Because RDDs are cached directly inside of executors, tasks can run alongside the cached data.
		
		In LOCAL mode, the Spark driver runs along with an executor in the same Java process. 
			This is a special case; executors typically each run in a dedicated process.

	Cluster Manager:
		
		Spark depends on a cluster manager to launch executors and, in certain cases, to launch the driver.
		The cluster manager is a pluggable component in Spark. (so, easy for others too, like mesos or yarn)
		
		Spark’s documentation consistently uses the terms driver and executor 	
			when describing the processes that execute each Spark application.
		The terms master and worker are used to describe the centralized 
			and distributed portions of the cluster manager.
		Spark can run both drivers and executors on the YARN worker nodes.
		
	Launching a Program:
	
		No matter which cluster manager you use, Spark provides a single script you can use
			to submit your program to it called spark-submit.
		Through various options, sparksubmit can connect to different cluster managers 
			and control how many resources your application gets.
		
	Summary

		To summarize the concepts in this section, let’s walk through the exact steps that
			occur when you run a Spark application on a cluster:
		1. The user submits an application using spark-submit.
		2. spark-submit launches the driver program and invokes the main() method specified by the user.
		3. The driver program contacts the cluster manager to ask for resources to launch executors.
		4. The cluster manager launches executors on behalf of the driver program.
		5. The driver process runs through the user application. Based on the RDD actions
			and transformations in the program, the driver sends work to executors in the form of tasks.
		6. Tasks are run on executor processes to compute and save results.
		7. If the driver’s main() method exits or it calls SparkContext.stop(), it will terminate
			the executors and release resources from the cluster manager.
			
	$ bin/spark-submit my_script.py
	
		When spark-submit is called with nothing but the name of a script or JAR, 
			it simply runs the supplied Spark program locally.
	
	$ bin/spark-submit --master spark://host:7077 --executor-memory 10g my_script.py
	
		spark:// URL means a cluster using Spark’s Standalone mode
		mesos:// -- for mesos
		yarn	 -- for yarn
		local 	 -- local
		local[n] -- local with N cores
		local[*] -- local with available cores in that machine
		
		--executor-memory
		--driver-memory
		The amount of memory to use for executors/drivers, in bytes. 
		Suffixes can be used to specify larger quantities such as “512m” (512 megabytes) or “15g” (15 gigabytes).

		examples:
		
		# Submitting a Java application to Standalone cluster mode

			$ ./bin/spark-submit \
			--master spark://hostname:7077 \
			--deploy-mode cluster \
			--class com.databricks.examples.SparkExample \
			--name "Example Program" \
			--jars dep1.jar,dep2.jar,dep3.jar \
			--total-executor-cores 300 \
			--executor-memory 10g \
			myApp.jar "options" "to your application" "go here"

		# Submitting a Python application in YARN client mode

			$ export HADOP_CONF_DIR=/opt/hadoop/conf
			$ ./bin/spark-submit \
			--master yarn \
			--py-files somelib-1.2.egg,otherlib-4.4.zip,other-file.py \
			--deploy-mode client \
			--name "Example Program" \
			--queue exampleQueue \
			--num-executors 40 \
			--executor-memory 10g \
			my_script.py "options" "to your application" "go here"

	it’s common practice to rely on a build tool to produce a single large JAR containing 
		the entire transitive dependency graph of an application. This is often called an 
		"uber JAR" or an "assembly JAR", and most Java or Scala build tools can produce this type of artifact.
		
	The most popular build tools for Java and Scala are Maven and sbt (Scala build tool).
		Either tool can be used with either language, but Maven is more often used for Java
		projects and sbt for Scala projects.

	Spark is marked as "provided" to ensure that Spark is never packaged with the application artifacts.
	The build includes the maven-shadeplugin to create an uber JAR containing all of its dependencies. 
		You enable this by asking Maven to execute the "shade" goal of the plug-in every time a "package" phase occurs.
	
	Dependency Conflicts:
		use differenct namespaces (or) use same version jar as spark uses (or) use shading
		
		
Scheduling Within and Between Spark Applications:
	
	Many clusters are shared between multiple users. Shared environments have the challenge of scheduling
	For scheduling in multitenant clusters, Spark primarily relies on the cluster manager 
		to share resources between Spark applications.
	









				
			
			
			
			
			
			
			
			
			
			








				
