Learning Spark:
--------------

##################################################################################################
#### Book: Learning Spark
#### Author: Holden Karau, Andy Konwinski, Patrick Wendell & Matei Zaharia
#### Available: Feb 2015
#### Spark Version Used: 1.1.0
#### 
#### IRead: Mar 2016 (after 1 year), Duration: xx Days took to finish
#### Spark Version Available: 1.6.1
#### 
#### JT Rating: x.x/5
#### Book Focused on: 
#### 
#### Chapters to Ignore: (if any)
##################################################################################################

############################MY-TRACKS#############################################################
#### CH_1_DONE => 3 hours (21-MAR-2016)
#### CH_2_DONE => 2 hours (21-MAR-2016)
#### CH_3_DONE => 5 hours (21-MAR-2016)
#### CH_4_DONE => 8 hours (22-MAR-2016)


##################################################################################################

***************************************************************************************************
****************************************CHAPTER-ONE************************************************
***************************************************************************************************

CHP-1-"Introduction to Data Analysis with Spark":

What Is Apache Spark?
Ans:
	Apache Spark is a cluster computing platform designed to be fast and generalpurpose.
	
Why Is Apache Spark?
Ans:
	Spark extends the popular MapReduce model to efficiently support
		more types of computations, including interactive queries and stream processing.
	Spark runs on Memory as well as Disk.
	Spark is designed to cover a wide range of workloads that previously
		required separate distributed systems, including batch applications, iterative
		algorithms, interactive queries, and streaming.
	Spark is designed to be highly accessible, offering simple APIs in Python, Java, Scala,
		and SQL, and rich built-in libraries.
	Spark can run in Hadoop clusters and access any Hadoop data	source, including Cassandra.	

A Unified Stack: 
	Spark project contains multiple closely integrated components.

Spark Core:
	At its core, Spark
		is a “computational engine” that is responsible for scheduling, distributing, and monitoring
		applications consisting of many computational tasks across many worker
		machines, or a computing cluster.
	Spark Core is also home to the API that defines resilient distributed datasets
		(RDDs), which are Spark’s main programming abstraction.
		
Spark SQL:
	Spark SQL was added to Spark in version 1.0.
	Spark SQL supports many sources of data, including Hive tables, Parquet, and JSON.
	Spark SQL allows developers
		to intermix SQL queries with the programmatic data manipulations supported by
		RDDs in Python, Java, and Scala, all within a single application, thus combining SQL
		with complex analytics.
	
	Shark:
		Shark was an older SQL-on-Spark project out of the University of California, Berkeley,
			that modified Apache Hive to run on Spark. It has now been replaced by Spark
			SQL to provide better integration with the Spark engine and language APIs.
		
Spark Streaming:

	Spark Streaming is a Spark component that enables processing of live streams of data.	
	Spark Streaming provides an API for manipulating data streams that closely matches the Spark Core’s RDD API.
	Underneath its API, Spark Streaming was designed to provide the
		same degree of fault tolerance, throughput, and scalability as Spark Core.
	
MLlib:

	Spark comes with a library containing common machine learning (ML) functionality, called MLlib.
	MLlib provides multiple types of machine learning algorithms, including
		classification, 
		regression, 
		clustering, 
		and collaborative filtering, 
		as well as supporting functionality 
			such as model evaluation 
			and data import.
	All of these methods are designed to scale out across a cluster.
	
GraphX:

	GraphX is a library for manipulating graphs and performing graph-parallel computations.
	GraphX also provides various operators
		for manipulating graphs (e.g., subgraph and mapVertices) and a library of
		common graph algorithms (e.g., PageRank and triangle counting).
	
Cluster Managers:
	
	Spark is designed to efficiently scale up from one to many thousands of compute nodes.
	Spark can run over a variety of cluster managers, 
		including Hadoop YARN, Apache Mesos, and a simple cluster manager included 
		in Spark itself called the Standalone Scheduler.
		
Who Uses Spark, and for What?
	
Data Science Tasks:
	
	While there is no standard definition, for our purposes a data scientist
		is somebody whose main task is to analyze and model data. Data scientists may have
		experience with SQL, statistics, predictive modeling (machine learning), and programming,
		usually in Python, Matlab, or R.

	Data scientists use their skills to analyze data with the goal of answering a question or
		discovering insights.
		
	Spark enables data scientists to tackle problems with larger data sizes than they could before
		with tools like R or Pandas. There is support for calling out to external programs in Matlab or R

	For example, the initial investigation of a data scientist might lead to the creation of a 
		production recommender system that is integrated into a web application and used to generate product suggestions to users. 
	
	Often it is a different person or team that leads the process of productizing the work of the data scientists,
		and that person is often an engineer.

Data Processing Applications: All other things like, to integrate them into production systems.
	
History of Spark:
	
	Unk, 2009 - Started at UC Berkeley
	Mar, 2010 - Open Sourced
	Jun, 2013 - Transferred to Apache Software Foundation (ASF)
	
Storage Layers for Spark:
	
	Spark can create distributed datasets from any file stored in the Hadoop distributed filesystem (HDFS) 
		or other storage systems supported by the Hadoop APIs 
			(including your local filesystem, 
							Amazon S3, 
							Cassandra, 
							Hive, 
							HBase, etc.).
	It’s important to remember that Spark does not require Hadoop; 
		it simply has support for storage systems implementing the Hadoop APIs.

	Spark supports 
		text files, 
		SequenceFiles, 
		Avro, 
		Parquet, 
		and any other Hadoop InputFormat.

***************************************************************************************************
****************************************CHAPTER-TWO************************************************
***************************************************************************************************

CHP-2-"Downloading Spark and Getting Started":

	Spark can be used from Python, Java, or Scala.
	Spark itself is written in Scala, and runs on the Java Virtual Machine (JVM).
	Spark does not yet work with Python 3.
	
	This chapter will be with Spark running in local mode
	
	Spark can run in a variety of different modes, or environments. 
		Beyond local mode, Spark can also be run on 
		Mesos, 
		YARN, 
		or the Standalone Scheduler included in the Spark distribution.

	Unlike most other shells, however, which let you manipulate data using the disk and
		memory on a single machine, Spark’s shells allow you to interact with data that is distributed
		on disk or in memory across many machines, and Spark takes care of automatically
		distributing this processing.	
		
	Because Spark can load data into memory on the worker nodes, many distributed
		computations, even ones that process terabytes of data across dozens of machines,
		can run in a few seconds.

Start Python Shell:

	bin/pyspark
	
Start Scala Shell:

	bin/spark-shell
	
	You may find the logging statements that get printed in the shell distracting. 
	You can control the verbosity of the logging. To do this, you can create a file in the conf directory called log4j.properties. 
	The Spark developers already include a template for this file called log4j.properties.template.

	To make the logging less verbose, make a copy of
		conf/log4j.properties.template called conf/log4j.properties and find the following line:

		log4j.rootCategory=INFO, console
		
		convert to
		
		log4j.rootCategory=WARN, console

	Using IPython:
		
		IPython is an enhanced Python shell that many Python users prefer,
			offering features such as tab completion. You can find instructions
			for installing it at http://ipython.org.

		SET IPYTHON environment variable
			IPYTHON=1 ./bin/pyspark
			
		To use the IPython Notebook, which is a web-browser-based version of IPython, use:
			IPYTHON_OPTS="notebook" ./bin/pyspark
			
RDD: Resilient Distributed Datasets

	In Spark, we express our computation through operations on distributed collections
		that are automatically parallelized across the cluster. 
	These collections are called resilient distributed datasets, or RDDs. 
	RDDs are Spark’s fundamental abstraction for distributed data and computation.
			
	http://[ipaddress]:4040. 
		You can access the Spark UI there and see all sorts of information about your tasks and cluster.

Introduction to Core Spark Concepts:

	At a high level, 
		every Spark application consists of a driver program that launches various parallel operations on a cluster.

	The driver program contains your application’s main function 
		and defines distributed datasets on the cluster, then applies operations to them.
		
	Here, the driver program was the Spark shell itself, 
		and you could just type in the operations you wanted to run.
		
	Driver programs access Spark through a SparkContext object, 
		which represents a connection to a computing cluster. 
		In the shell, a SparkContext is automatically created for you as the variable called "sc".
		
	Once you have a SparkContext, you can use it to build RDDs.

	To run these operations, driver programs typically manage a number of nodes called "executors".
		
	Finally, a lot of Spark’s API revolves around passing functions to its operators to run them on the cluster	
		
	While we will cover the Spark API in more detail later, 
		a lot of its magic is that function-based operations like filter also parallelize across the cluster.
		
	That is, Spark automatically takes your function (e.g., line.contains("Python")) 
		and ships it to executor nodes. 
	Thus, you can write code in a single driver program and automatically have parts of it run on multiple nodes.
		
	"Passing Functions to Spark"	: more details on other notes of mine.
		If you are unfamiliar with the lambda or => syntax, 
			it is a shorthand way to define functions inline in Python and Scala.
		When using Spark in these languages, 
			you can also define a function separately and then pass its name to Spark.
		Passing functions to Spark is also possible in Java, 
			but in this case they are defined as classes, implementing an interface called Function.

		Java 8 introduces shorthand syntax called lambdas that looks similar to Python and Scala.

Standalone Applications: (not interactive shells, code in files, ".java" like)

	Apart from running interactively, Spark can be linked into standalone applications
		in either Java, Scala, or Python.
	The main difference from using it in the shell is that you need to initialize your own SparkContext. 
		After that, the API is the same.

	Eclipse allow you to directly add a Maven dependency to a project.
	
	In Python, you simply write applications as Python scripts, but you must run them
		using the "bin/spark-submit" script included in Spark.
		
	The spark-submit script includes the Spark dependencies for us in Python.

	Once you have linked an application to Spark, you need to import the Spark packages in your program and create a SparkContext. 
		You do so by first creating a "SparkConf" object to configure your application, and then building a "SparkContext" for it.

	Example 2-8. Initializing Spark in Scala
		import org.apache.spark.SparkConf
		import org.apache.spark.SparkContext
		import org.apache.spark.SparkContext._

		val conf = new SparkConf().setMaster("local").setAppName("My App")
		val sc = new SparkContext(conf)

	A "cluster URL", namely "local" in these examples, which tells Spark how to connect to a cluster. 
		"local" is a special value that runs Spark on one thread on the local machine, without connecting to a cluster.

	An "application name", namely "My App" in these examples. 
		This will identify your application on the cluster manager’s UI if you connect to a cluster.

	Additional parameters exist for configuring how your application executes or adding code to be shipped to the cluster
	After you have initialized a SparkContext, 
		you can use all the methods we showed before to create RDDs (e.g., from a text file) and manipulate them.
	Finally, to shut down Spark, you can either call the stop() method on your SparkContext, 
		or simply exit the application (e.g., with System.exit(0) or sys.exit()).

Build:

	We’ve marked the Spark Core dependency as provided so that, 
		later on, when we use an assembly JAR we don’t include the spark-core JAR, which is already on the classpath of the workers.

	<project>
		<groupId>com.oreilly.learningsparkexamples.mini</groupId>
		<artifactId>learning-spark-mini-example</artifactId>
		<modelVersion>4.0.0</modelVersion>
		<name>example</name>
		<packaging>jar</packaging>
		<version>0.0.1</version>
		<dependencies>
			<dependency> <!-- Spark dependency -->
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-core_2.10</artifactId>
			<version>1.2.0</version>
			<scope>provided</scope>
			</dependency>
		</dependencies>
		<properties>
			<java.version>1.6</java.version>
		</properties>
		<build>
			<pluginManagement>
			<plugins>
				<plugin>
					<groupId>org.apache.maven.plugins</groupId>
					<artifactId>maven-compiler-plugin</artifactId>
					<version>3.1</version>
					<configuration>
						<source>${java.version}</source>
						<target>${java.version}</target>
					</configuration> 
				</plugin>
			</plugins>
			</pluginManagement>
		</build>
	</project>

	The spark-core package is marked as "provided" in case we package our application into an assembly JAR.

	$ mvn clean && mvn compile && mvn package
	
Run:
	
	$ $SPARK_HOME/bin/spark-submit \
	--class com.oreilly.learningsparkexamples.mini.java.WordCount \
	./target/learning-spark-mini-example-0.0.1.jar \
	./README.md ./wordcounts

	A driver program 
		creates a SparkContext and RDDs, 
		and then runs parallel operations on them

***************************************************************************************************
**************************************CHAPTER-THREE************************************************
***************************************************************************************************

CHP-3-"Programming with RDDs":

	An RDD is simply a distributed collection of elements.
	
	In Spark all work is expressed as either 
		(1) creating new RDDs, 
		(2) or transforming existing RDDs, 
		(3) or calling operations on RDDs to compute a result.

	Under the hood, Spark automatically 
		(1) distributes the data contained in RDDs across your cluster 
		(2) and parallelizes the operations you perform on them.

	An RDD in Spark is simply an "immutable" distributed collection of objects.
		
	Each RDD is 
		split into multiple "partitions", 
		which may be computed on different nodes of the cluster. 
	RDDs can contain 
		any type of Python, Java, or Scala objects, 
		including userdefined classes.
		
	Users create RDDs in two ways: 
		(1) by loading an external dataset,  [	val abc = SparkContext.textFile(/abc.txt)	]
		(2) or by distributing a collection of objects (e.g., a list or set) in their driver program. 
			[	val lines = sc.parallelize(List("pandas", "i like pandas"))	], not widely used, as it need's entire data in memory.
		
	Once created, 
		RDDs offer two types of operations: 
			(1) transformations 
			(2) and actions.
		
		Transformations construct a new RDD from a previous one. [i.e. it returns another RDD]
			For example, one common transformation is filtering data that matches a literal string(AKA predicate).
		
		Actions, on the other hand, compute a result based on an RDD, 
			and either return it to the driver program 
			or save it to an external storage system (e.g., HDFS).
			One example of an action we called earlier is "first()", which returns the first element in an RDD
		
	Transformations and actions are different 
		because of the way Spark computes RDDs.
	Although you can define new RDDs any time, Spark computes them only in a lazy fashion, 
		that is, the first time they are used in an action.
	This approach might seem unusual at first, 
		but makes a lot of sense when you are working with Big Data.	
		
	If Spark were to load and store all the lines in the file as soon as we wrote lines = sc.textFile(...), 
		it would waste a lot of storage space, given that we then immediately filter out many lines. 
	Instead, once Spark sees the whole chain of transformations, it can compute just the data needed for its result. 
	
	In fact, for the first() action, 
		Spark scans the file only until it finds the first matching line; it doesn’t even read the whole file.	[OPTIMIZATION]
		
	Finally, Spark’s RDDs are by default recomputed each time you run an action on them.
		If you would like to reuse an RDD in multiple actions, you can ask Spark to persist it using RDD.persist().

	After computing it the first time,
		Spark will store the RDD contents in memory (partitioned across the machines in your cluster), 
		and reuse them in future actions.
	Persisting RDDs on disk instead of memory is also possible.

	The behavior of not persisting by default may again seem unusual, 
		but it makes a lot of sense for big datasets: 
		if you will not reuse the RDD,there’s no reason to waste storage space 
		when Spark could instead stream through the data once and just compute the result.

	To summarize, every Spark program and shell session will work as follows:
		1. Create some input RDDs from external data.
		2. Transform them to define new RDDs using transformations like filter().
		3. Ask Spark to persist() any intermediate RDDs that will need to be reused.
		4. Launch actions such as count() and first() to kick off a parallel computation,
			which is then optimized and executed by Spark.

	cache() is the same as calling persist() with the "default storage level".

RDD Operations:
	
	Transformations are operations on RDDs that return a new RDD, such as map() and filter().

	Actions are operations that 
		return a result to the driver program 
		or write it to storage, 
		and kick off a computation, such as count() and first().

	If you are ever confused whether a given function is a transformation or an action, 
		you can look at its return type: transformations return RDDs, whereas actions return some other data type.

Transformations:

	Transformations are operations on RDDs that return a new RDD. 
	As discussed in “Lazy Evaluation”, 
		transformed RDDs are computed lazily, only when you use them in an action. 
	Many transformations are element-wise; that is, they work on one element at a time; 
		but this is not true for all transformations.
	
	Example 3-12. filter() transformation in Scala
		val inputRDD = sc.textFile("log.txt")
		val errorsRDD = inputRDD.filter(line => line.contains("error"))
			
	Note that the filter() operation does not mutate the existing "inputRDD". 
		Instead, it returns a pointer to an entirely new RDD.

	Transformations can actually operate on any number of input RDDs.
		for example, union() is a bit different than filter(), in that it operates on two RDDs instead of one.

	Finally, as you derive new RDDs from each other using transformations,
		Spark keeps track of the set of dependencies between different RDDs, 
		called the "lineage graph".

	It uses this information to compute each RDD on demand 
		and to recover lost data if part of a persistent RDD is lost.

Actions:

	Actions are the second type of RDD operation.
	Actions force the evaluation of the transformations required for the RDD 
		they were called on, since they need to actually produce output.

	Example 3-16. Scala error count using actions
		println("Input had " + badLinesRDD.count() + " concerning lines")
		println("Here are 10 examples:")
		badLinesRDD.take(10).foreach(println)

	We used take() to retrieve a small number of elements in the RDD at the driver program. 
		We then iterate over them locally to print out information at the driver.
	
	RDDs also have a collect() function to retrieve the entire RDD.
		This can be useful if your program filters RDDs down to a very small size and you’d like to deal with it locally.
	
	Keep in mind that your entire dataset must fit in memory on a single machine to use collect() on it, 
		so collect() shouldn’t be used on large datasets.

	In most cases RDDs can’t just be collect()ed to the driver because they are too large. 
	In these cases, it’s common to write data out to a 
		distributed storage system 
			such as HDFS 
			or Amazon S3.
	You can save the contents of an RDD using the saveAsTextFile() action, saveAsSequenceFile(), 
		or any of a number of actions for various built-in formats.

	It is important to note that each time we call a new action, the entire RDD must be computed “from scratch.” 
	To avoid this inefficiency, users can persist intermediate results.
	
Lazy Evaluation:
	
	As you read earlier, 
		TRANSFORMATIONS on RDDs are lazily evaluated, 
		meaning that,
		Spark will not begin to execute until it sees an ACTION.

	Lazy evaluation means that when we call a transformation on an RDD (for instance, calling map()), 
		the operation is not immediately performed. 
		Instead, Spark internally records metadata to indicate that this operation has been requested.

	Rather than thinking of an RDD 
		as "containing specific data", 
			it is best to think of each RDD as
		"consisting of instructions" on how to compute the data that we build up through transformations.

	Loading data into an RDD is lazily evaluated in the same way transformations are. 
	So, when we call sc.textFile(), the data is not loaded until it is necessary.

	Although transformations are lazy, you can force Spark to execute them at any time by running an action, such as count(). 
	This is an easy way to test out just part of your program.
	
	Spark uses lazy evaluation to reduce the number of passes it has to take over our data
		by grouping operations together.
		
	In systems like Hadoop MapReduce, developers often have to spend a lot of time 
		considering how to group together operations to minimize the number of MapReduce passes.
	In Spark, there is no substantial benefit to writing a single complex map instead of chaining together many simple operations.
	
	Thus, users are free to organize their program into smaller, more manageable operations.

Passing Functions to Spark:

	Most of Spark’s transformations, and some of its actions, depend on passing in functions
		that are used by Spark to compute data.

	Types of "Passing Functions"
	
		Python:
		(1) pass lambda expressions
		(2) pass in top-level functions
		(3) pass locally defined functions
		
		One issue to watch out for when passing functions is inadvertently serializing the object containing the function.
		When you pass a function that is the member of an object, or contains references to fields in an object (e.g., self.field), 
			Spark sends the entire object to worker nodes, which can be much larger than the bit of information you need.
		
		Sometimes this can also cause your program to fail, 
			if your class contains objects that Python can’t figure out how to pickle.
		
		Scala:
		(1) pass in functions defined inline
		(2) pass references to methods
		(3) pass static functions
		
		The function we pass and the data referenced in it needs to be serializable (implementing Java’s Serializable interface).

		If NotSerializableException occurs in Scala, a reference to a method or field in a nonserializable class 
			is usually the problem. Note that passing in local serializable variables 
			or functions that are members of a top-level object is always safe.
		
		Java:
		In Java, functions are specified as objects that 
			implement one of Spark’s function interfaces from the org.apache.spark.api.java.function package.
		
		There are a number of different interfaces based on the return type of the function.
		
		TABLE of COMMON Functions:
		
			Function name				Method to implement				Example
			-------------				-------------------				-------
			Function<T, R>				R call(T)						map() or filter()
			Function2<T1, T2, R>		R call(T1, T2)					aggregate() or fold()
			FlatMapFunction<T, R>		Iterable<R> call(T)				flatMap()
		
		We can either define our function classes 
			(1) inline as anonymous inner classes or
			(2) or create a named class
			
		Example 3-22. Java function passing with anonymous inner class
			RDD<String> errors = lines.filter(new Function<String, Boolean>() {
				public Boolean call(String x) { return x.contains("error"); }
			});

		Example 3-23. Java function passing with named class
			class ContainsError implements Function<String, Boolean>() {
				public Boolean call(String x) { return x.contains("error"); }
			}
			
			RDD<String> errors = lines.filter(new ContainsError());
		
		Benefits of TOP LEVEL named functions:
			(1) are often cleaner for organizing large programs
			(2) can give them constructor parameters [in other words, can avoid hard coding like above]
		
		Example 3-24. Java function class with parameters
			class Contains implements Function<String, Boolean>() {
				private String query;
				public Contains(String query) { this.query = query; }
				public Boolean call(String x) { return x.contains(query); }
			}
				
			RDD<String> errors = lines.filter(new Contains("error"));		
		
		Using LAMBDA expressions, JAVA 8, more simple:
		
			RDD<String> errors = lines.filter(s -> s.contains("error"));
		
Common Transformations and Actions:

	Basic RDDs:
	
		Element-wise transformations:
		
		map() 
			transformation takes in a function and "applies it to each element in the RDD" 
				with the result of the function being the new value of each element in the resulting RDD.
		
		filter() 
			transformation takes in a function and returns an RDD 
				that only has elements that pass the filter() function.
		
		Change RDD type by using map().
		We can use map() to do any number of things, from fetching the website associated
			with each URL in our collection to just squaring the numbers.
		It is useful to note that map()’s return type does not have to be the same as its input type, 
			so if we had an RDD String and our map() function were to parse the strings and return a Double,
			our input RDD type would be RDD[String] and the resulting RDD type would be RDD[Double].

		flatMap()
			Sometimes we want to produce multiple output elements for each input element.[in map() one output for one input]
			Instead of returning a single element, we return an iterator with our return values.
			Rather than producing an RDD of iterators, we get back an RDD that consists of the elements
				from all of the iterators.

		Example 3-31. flatMap() in Java, splitting lines into multiple words
			JavaRDD<String> lines = sc.parallelize(Arrays.asList("hello world", "hi"));
			JavaRDD<String> words = lines.flatMap(new FlatMapFunction<String, String>() {
				public Iterable<String> call(String line) {
					return Arrays.asList(line.split(" "));
				}
			});
			words.first(); // returns "hello"
		
		Think of flatMap() as “flattening” the iterators returned to it, 
			so that instead of ending up with an RDD as "collection of small collections" 
			we will simply have a "big collection".
		
		Example with words:
		
		RDD 			=> {"coffee panda", "happy panda", "happiest panda party"}
		mappedRDD 		=> {["coffee", "panda"], ["happy", "panda"], ["happiest", "panda", "party"]}
		flatMappedRDD 	=> {"coffee", "panda", "happy", "panda", "happiest", "panda", "party"}

		Pseudo set operations:
		
			RDDs support many of the operations of mathematical sets, such as union and intersection,
				even when the RDDs themselves are not properly sets.
			All of these operations require that the RDDs being operated on are of the same type.

			RDD1.distinct()
			RDD1.union(RDD2)
			RDD1.intersection(RDD2)
			RDD1.subtract(RDD2)
			RDD1.cartesian(RDD2)

			If we want only unique elements we can use the RDD.distinct() transformation 
				to produce a new RDD with only distinct items. distinct() is COSTLY.
				
			Spark’s union() will contain duplicates.
			Spark’s intersection() also removes all duplicates.
			
			intersection(), subtract(), distinct() uses SHUFFLE operation on network, its costly.

	Actions:
	
	reduce()
	With reduce(), we can easily sum the elements of our RDD, 
		count the number of elements, and perform other types of aggregations

	fold()
	similar to reduce(), in addition takes a “zero value” to be used for the initial call on each partition.
	The zero value you provide should be the identity element for your operation; 
		that is, applying it multiple times with your function should not change the value 
		(e.g., 0 for +, 1 for *, or an empty list for concatenation).

	Both fold() and reduce() require that the return type of our result be the same type
		as that of the elements in the RDD we are operating over.

	aggregate()
		function frees us from the constraint of having the return be the same type as the RDD we are working on.

	Example 3-37. aggregate() in Java
		class AvgCount implements Serializable {
			public AvgCount(int total, int num) {
			this.total = total;
			this.num = num;
			}
			public int total;
			public int num;
			public double avg() {
				return total / (double) num;
			}
		}
		Function2<AvgCount, Integer, AvgCount> addAndCount =
			new Function2<AvgCount, Integer, AvgCount>() {
			public AvgCount call(AvgCount a, Integer x) {
			a.total += x;
			a.num += 1;
			return a;
			}
		};
		Function2<AvgCount, AvgCount, AvgCount> combine =
			new Function2<AvgCount, AvgCount, AvgCount>() {
			public AvgCount call(AvgCount a, AvgCount b) {
			a.total += b.total;
			a.num += b.num;
			return a;
			}
		};

		AvgCount initial = new AvgCount(0, 0);
		AvgCount result = rdd.aggregate(initial, addAndCount, combine);
		System.out.println(result.avg());

		IN OTHER WORDS: 
			initial, addAndCount, combine 	=> first-element(0,0), next-element, aggregate-of-both-elements
											=> previous-element, next-element, aggregate-of-both-elements


		The simplest and most common operation that returns data to our driver program is collect(), 
			which returns the entire RDD’s contents. Problem is data need to fit in memory.
		And, take(n) returns n elements from the RDD and attempts to minimize the number of
			partitions it accesses, so it may represent a biased collection.
		It’s important to note that these operations do not return the elements in the order you might expect.
		These operations are useful for unit tests and quick debugging, but may introduce
			bottlenecks when you’re dealing with large amounts of data.
			
		top()
			will use the default ordering on the data, 
				but we can supply our own comparison function to extract the top elements.
		
		takeSample(withReplacement, num, seed) 
			function allows us to take a sample of our data either with or without replacement.

		foreach()
			an action without returning anything.
			Sometimes it is useful to perform an action on all of the elements in the RDD, 
				but without returning any result to the driver program. 
			A good example of this would be posting JSON to a webserver or inserting records into a database.
	
		count()
			returns a count of the elements
			
		countByValue() 
			returns a map of each unique value to its count

Converting Between RDD Types:
	
	Some functions are available only on certain types of RDDs,
		such as mean() and variance() on numeric RDDs
		or join() on key/value pair RDDs.
		
	In Scala and Java, these methods aren’t defined on the standard RDD class, 
		so to access this additional functionality we have to make sure we get the correct specialized class.
	
	SCALA:
	In Scala the conversion to RDDs with special functions is handled automatically using implicit conversions.
	These implicits turn an RDD into various wrapper classes, such as DoubleRDDFunctions (for RDDs of numeric data) 
		and PairRDDFunctions (for key/value pairs), to expose additional functions such as mean() and variance().
	
	Implicits, while quite powerful, can sometimes be confusing. 
	If you call a function like mean() on an RDD, you might look at the Scaladocs for the RDD class and notice
		there is no mean() function. 
	The call manages to succeed because of implicit conversions between RDD[Double] and DoubleRDDFunctions. 
	When searching for functions on your RDD in Scaladoc, 
		make sure to look at functions that are available in these wrapper classes.
	
	JAVA:
	In Java the conversion between the specialized types of RDDs is a bit more explicit.
	In particular, there are special classes called JavaDoubleRDD and JavaPairRDD for RDDs of these types, 	
		with extra methods for these types of data. we can understand easily.
	
	To construct RDDs of these special types, instead of always using the Function class
		we will need to use specialized versions.
	
	If we want to create a DoubleRDD from an RDD of type T, 
		rather than using "Function<T, Double>" we use "DoubleFunction<T>".
	We also need to call different functions on our RDD 
		(so we can’t just create a DoubleFunction and pass it to map()).
	When we want a DoubleRDD back, instead of calling map(), we need to call mapToDouble() 
		with the same pattern all of the other functions
	
	PYTHON:
	The Python API is structured differently than Java and Scala. In Python all of the functions are implemented 
		on the base RDD class but will fail at runtime if the type of data in the RDD is incorrect.
	
Persistence (Caching):
	
	When we ask Spark to persist an RDD, the nodes that compute the RDD store their partitions.
	If a node that has data persisted on it fails, Spark will recompute the lost partitions of the data when needed.
	We can also replicate our data on multiple nodes if we want to be able to handle node failure without slowdown.

	Recomputing an entire RDD is especially expensive for iterative algorithms, which look at the data many times.
		A trivial example would be doing a count and then writing out the same RDD.

	Example 3-39. Double execution in Scala
		val result = input.map(x => x*x)
		println(result.count())
		println(result.collect().mkString(","))

	Spark has many levels of persistence to choose from based on what our goals are:
		In Scala and Java,
			the default persist() will store the data in the JVM heap as unserialized objects.
		In Python,
			we always serialize the data that persist stores, 
				so the default is instead stored in the JVM heap as pickled(serialized) objects.
			When we write data out to disk or off-heap storage, that data is also always serialized.

	[[Off-heap caching is experimental and uses Tachyon. If you are
	interested in off-heap caching with Spark, take a look at the Running
	Spark on Tachyon guide.]]

	The persist() call on its own doesn’t force evaluation.
	
	If you attempt to cache too much data to fit in memory, Spark will automatically
		evict(spill) old partitions using a Least Recently Used (LRU) cache policy.
	For the memoryonly storage levels, 
		it will recompute these partitions the next time they are accessed.
	For the memory-and-disk ones, it will write them out to disk.

	In either case, this means that you don’t have to worry about your job breaking if you ask Spark to cache too much data.
	However, caching unnecessary data can lead to eviction of useful data and more recomputation time.
	
***************************************************************************************************
**************************************CHAPTER-FOUR*************************************************
***************************************************************************************************

CHP-4-"Working with Key/Value Pairs":

	Key/Value pairs are common data type required for many operations in Spark.
	They are used to perform aggregations, or some new operations
		and often we will do some initial ETL to get our data into a key/value format.
	Using controllable partitioning, applications can sometimes greatly reduce communication costs 
		by ensuring that data will be accessed together and will be on the same node.

	Choosing the right partitioning for a distributed dataset is similar 
		to choosing the right data structure for a local one—in both cases, data layout can greatly affect performance.

	Spark provides special operations on RDDs containing key/value pairs. 
	These RDDs are called "pair RDDs".
	
	pair RDDs have,
		reduceByKey(), method that can aggregate data separately for each key
		join(), method that can merge two RDDs together by grouping elements with the same key
	
	It is common to extract fields from an RDD and use those fields as keys in pair RDD operations.

Creating Pair RDDs:

	There are a number of ways to get pair RDDs in Spark.
	Many input formats at loading time will directly return pair RDDs for their key/value data.
	We can turn a "regular RDD" into "pair RDD". [by using map()]
	
	The way to build key-value RDDs differs by language.
	
	Python:
	The functions on keyed data to work we need to return an RDD composed of tuples
	pairs = lines.map(lambda x: (x.split(" ")[0], x))

	Scala:
	The functions on keyed data to work we also need to return an RDD composed of tuples
	val pairs = lines.map(x => (x.split(" ")(0), x))
	
	Java:
	Java doesn’t have a built-in tuple type,
		so Spark’s Java API helps users to create tuples using the scala.Tuple2 class.
	Java users can construct a new tuple by writing "new Tuple2(elem1, elem2)"
	And can then access its elements with the "._1()" and "._2()" methods.

	Java users also need to call special versions of Spark’s functions when creating pairRDDs.
	For instance, 
		the mapToPair() function should be used in place of the basic map() function.

	PairFunction<String, String, String> keyData =
		new PairFunction<String, String, String>() {
		public Tuple2<String, String> call(String x) {
			return new Tuple2(x.split(" ")[0], x);
		}
	};
	JavaPairRDD<String, String> pairs = lines.mapToPair(keyData);

	When creating a pair RDD from an in-memory collection,
		In Scala and Python,
			we need to call "SparkContext.parallelize()"
		In Java,
			we need to call "SparkContext.parallelizePairs()"

Transformations on Pair RDDs:

	Pair RDDs are allowed to use all the transformations available to standard RDDs.
	Since pair RDDs contain tuples, 
		we need to pass functions that "operate on tuples" rather than on individual elements.

	Table 4-1. Transformations on one pair RDD
	Table 4-2. Transformations on two pair RDD
	
	Pair RDDs are also still RDDs (of Tuple2 objects in Java/Scala or of Python tuples),
		and thus support the same functions as RDDs.

	Example 4-6. Simple filter on second element in Java
		Function<Tuple2<String, String>, Boolean> longWordFilter =
			new Function<Tuple2<String, String>, Boolean>() {
				public Boolean call(Tuple2<String, String> keyValue) {
					return (keyValue._2().length() < 20);
			}
			};
		JavaPairRDD<String, String> result = pairs.filter(longWordFilter);

Aggregations:
	When datasets are described in terms of key/value pairs, it is common to want to
		aggregate statistics across all elements with the same key.
	These operations return RDDs and thus are transformations rather than actions.

	reduceByKey() is quite similar to reduce(); both take a function and use it to combine values.
	reduceByKey() runs several parallel reduce operations, one for each key in the dataset, 
		where each operation combines values that have the same key.

	foldByKey() is quite similar to fold();both use a zero value of the same type of the
		data in our RDD and combination function.
	
	Those familiar with the combiner concept from MapReduce should note that calling reduceByKey() and foldByKey() will
		automatically perform combining locally on each machine before computing global totals for each key. 
	The user does not need to specify a combiner. The more general combineByKey() interface 
		allows you to customize combining behavior
	
	WordCount in one line,
		val newRDD = input.flatMap(x => x.split(" ")).countByValue()

	combineByKey() is the most general of the per-key aggregation functions.
	Like aggregate(), combineBy Key() allows the user to return values that are not the same type as our input data.
	As combineByKey() goes through the elements in a partition, each element either has a key it hasn’t seen before 
		or has the same key as a previous element.

	//combiner is like a datastructure in spark, understand more from the below URL
		http://abshinn.github.io/python/apache-spark/2014/10/11/using-combinebykey-in-apache-spark/
		http://stackoverflow.com/questions/29246756/how-createcombiner-mergevalue-mergecombiner-works-in-combinebykey-in-spark-us
		
	combineByKey(),
		Need to provide 3 functions as args, every computation happens on VALUE part of RDD
			First, to set initial value, called as "createCombiner()"
			Second, to increment/accumulate the subsequent value, called as "mergeValue()"
			Third, to accumulate/merge the values/accumulators across partitions, called as "mergeCombiners()"
		Since each partition is processed independently, we can have multiple accumulators for the same key.
		
		Example 4-14. Per-key average using combineByKey() in Java [see this later]
		In any case, using one of the specialized aggregation functions in Spark can be much faster than the
			naive approach of grouping our data and then reducing it.

Tuning the level of parallelism:

How Spark decides how to split up the work?
Ans:
	Every RDD has a fixed number of partitions 
		that determine the degree of parallelism to use when executing operations on the RDD.
	When performing aggregations or grouping operations, we can ask Spark to use a specific number of partitions.
	Spark will always try to infer a sensible default value based on the size of your cluster, 
		but in some cases you will want to tune the level of parallelism for better performance.

How to tell the number of partitions to use explicitly?
Ans:
	By using a second parameter.
	Example 4-15. reduceByKey() with custom parallelism in Python
		data = [("a", 3), ("b", 4), ("a", 1)]
		sc.parallelize(data).reduceByKey(lambda x, y: x + y) # Default parallelism
		sc.parallelize(data).reduceByKey(lambda x, y: x + y, 10) # Custom parallelism

	repartition(),
		Which shuffles the data across the network to create a new set of partitions.
		Sometimes, we want to change the partitioning of an RDD outside the context of grouping and aggregation operations.
		It is a fairly expensive operation.
		
	coalesce(),
		optimized version of "repartition()"
		Used, to bring down the number of partitions, like many to few
		That allows avoiding data movement, but only if you are decreasing the number of RDD partitions.

How to verifiy partitions count?
Ans:
	In Java/Scala,
		rdd.partitions.size()
		
	In Python,
		rdd.getNumPartitions()

Grouping Data:

	groupByKey(),
		RDD consisting of keys of type K and values of type V, we get back an RDD of type [K, Iterable[V]].
		
	groupBy(),
		works on unpaired data
		or data where we want to use a "different condition" besides equality on the current key.
		It takes a function that it applies to every element in the source RDD and uses the result to determine the key.
		
	rdd.reduceByKey(func),
		produces same result as "rdd.groupByKey().mapValues(value => value.reduce(func))"
		but is more efficient as it "avoids the step of creating a list of values" for each key.

	cogroup(),
		we can group data sharing the same key from multiple RDDs.
		If two RDDs sharing the same key type, K, with the respective value types V and W 
			gives us back RDD[(K, (Iterable[V], Iterable[W]))]
		If one of the RDDs doesn’t have elements for a given key that is present in the other RDD, 
			the corresponding Iterable is simply empty
		Additionally, cogroup() can work on three or more RDDs at once.

Joins:
	
	join(),
		an inner join, usage: RDD1.join(RDD2)
		
	leftOuterJoin(other)
	rightOuterJoin(other)
	
	For Outer Joins,
		The value associated with each key in the result is a tuple of the value from the source RDD 
			and an Option (or Optional in Java) for the value from the other pair RDD.
		In Python, if a value isn’t present None is used; 
			and if the value is present the regular value, without any wrapper, is used.

		[[Optional is part of Google’s Guava library and represents a possibly missing value. 
			We can check isPresent() to see if it’s set, 
			and get() will return the contained instance provided data is present.]]

Sorting Data:

	sortByKey(),
		We can sort an RDD with key/value pairs provided that there is an ordering defined on the key
		Once we have sorted our data, any subsequent call on the sorted data 
			to collect() or save() will result in ordered data.
		Sometimes we want a different sort order entirely, 
			and to support this we can provide our own comparison function.
		Default is ascending order.
	
	Example 4-20. Custom sort order in Scala, "sorting integers as if strings"
		val input: RDD[(Int, Venue)] = ...
			implicit val sortIntegersByString = new Ordering[Int] {
			override def compare(a: Int, b: Int) = a.toString.compare(b.toString)
		}
		rdd.sortByKey()

Actions Available on Pair RDDs:

	As with the transformations, all of the traditional actions available on the base RDD
		are also available on pair RDDs.
		
	Additional Actions,
		countByKey(), Count the number of elements for each key.(a wordcount kind of result)
		collectAsMap(), Collect the result as a map to provide easy lookup. (like dictionary, lookup table)
		lookup(key), Return all values associated with the provided key.

Data Partitioning (Advanced):
	
	partitionBy(),
		a TRANSFORMATION, to Control datasets partitioning across nodes.
		that means,
			it always returns a new RDD
			it does not change the original RDD
	
	In a distributed program, communication is very expensive,
		so laying out data to minimize network traffic can greatly improve performance.
	Partitioning is useful only when a dataset is reused multiple times in key-oriented operations such as joins.
	If a given RDD is scanned only once, there is no point in partitioning it in advance.
	
	Spark’s partitioning is available on all RDDs of key/value pairs, 
		and causes the system to group elements based on a function of each key.
	Although Spark does not give explicit control of which worker node each key goes to (to support fault tolarence), 
		it lets the program ensure that a set of keys will appear together on some node.
	For example, 
		you might choose to hashpartition an RDD into 100 partitions 
			so that keys that have the same hash value modulo 100 appear on the same node. 
		Or you might range-partition the RDD into sorted ranges of keys 
			so that elements with keys in the same range appear on the same node.
	Explore, Example 4-22 later as usecase is good.
	
	Example 4-23. Scala custom partitioner
		val sc = new SparkContext(...)
		val userData = sc.sequenceFile[UserID, UserInfo]("hdfs://...")
			.partitionBy(new HashPartitioner(100)) // Create 100 partitions
			.persist()
	
	Because we called partitionBy() when building userData, Spark will now know that it is hash-partitioned, 
		and calls to join() on it will take advantage of this information.
	The result is that a lot less data is communicated over the network, and the program runs significantly faster. 
	
	And you make sure that the NUM-OF-PARTITIONS at least as large as the number of cores in your cluster.
	Without persistence, use of the partitioned RDD will cause reevaluation of the RDDs complete lineage. 
		That would negate the advantage of partitionBy(), 
		resulting in repeated partitioning and shuffling of data across the network

	Many other Spark operations automatically result in an RDD with known partitioning information
	sortByKey(), 
		result in range-partitioned
	groupByKey()
		result in hash-partitioned
	map(),
		ignores parent RDD partition information, as the key changes, so no use

	In Python, you cannot pass a Hash Partitioner object to partitionBy;
		instead, you just pass the number of partitions desired (e.g., rdd.partitionBy(100)).

Determining an RDD’s Partitioner:
	
	spark.Partitioner object,
		The "partitioner" property is a great way to test in the Spark shell how different
			Spark operations affect partitioning.
	
	Example 4-24. Determining partitioner of an RDD, shows how to use partitionsBy() and partitioner property
		scala> val pairs = sc.parallelize(List((1, 1), (2, 2), (3, 3)))
		pairs: spark.RDD[(Int, Int)] = ParallelCollectionRDD[0] at parallelize at <console>:12

		scala> pairs.partitioner
		res0: Option[spark.Partitioner] = None

		scala> val partitioned = pairs.partitionBy(new spark.HashPartitioner(2))
		partitioned: spark.RDD[(Int, Int)] = ShuffledRDD[1] at partitionBy at <console>:14

		scala> partitioned.partitioner
		res1: Option[spark.Partitioner] = Some(spark.HashPartitioner@5147788d)
	
	If we actually wanted to use partitioned in further operations, 
		then we should have appended persist() to the third line of input, in which partitioned is defined.
	Without persist(), subsequent RDD actions will "evaluate the entire lineage" of partitioned, 
		which will cause pairs to be hash-partitioned "over and over".
	
Operations That Benefit from Partitioning:
	
	Save SHUFFLING, which is costly.
	
	Many of Spark’s operations involve shuffling data by key across the network.
	For operations that act on a single RDD, such as reduceByKey(), 
		running on a prepartitioned RDD will cause all the values for each key to be computed locally on a single machine, 
		requiring only the final, locally reduced value to be sent from each worker node back to the master.
	Finally, for binary operations, 
		which partitioner is set on the output depends on the parent RDDs’ partitioners. 
	By default, it is a hash partitioner, with the number of partitions set to the level of parallelism of the operation.
	
Example: PageRank

	The PageRank algorithm, named after Google’s Larry Page,
		aims to assign a measure of importance (a “rank”) to each document in a set 
		based on how many documents have links to it.
	
	PageRank is an iterative algorithm that performs many joins, so it is a good use case for RDD partitioning.
	
	// i need to spend more time to understand the PAGE RANK example in detail
	
Custom Partitioners:	
	
	Spark also allows you to tune how an RDD is partitioned by providing a custom Partitioner object.
	This can help you further reduce communication by taking advantage of domain-specific knowledge.
	
	Make a custom partitioner for PAGE RANK.
	As we know that web pages within the same domain tend to link to each other a lot.
		(e.g., http://www.cnn.com/WORLD and http://www.cnn.com/US)
	or else they will be in different partitions as HASH is different for them.
	
	To implement a custom partitioner, you need to subclass 
		the org.apache.spark.Partitioner class and implement three methods:
		(1) numPartitions: Int, which returns the number of partitions you will create
		(2) getPartition(key: Any): Int, which returns the partition ID (like R0 to Rn-1 in hadoop)
		(3) equals(),
			This is important to implement because Spark will need to test your Partitioner object against other instances
				of itself when it decides whether two of your RDDs are partitioned the same way!
		gotcha,
			Java’s hashCode() method returns negative integer values also.
			make sure you override it to get always positive integers.
			
		[[Note that the hash function you pass will be compared by identity to that of other
			RDDs. If you want to partition multiple RDDs with the same partitioner, pass the
			same function object (e.g., a global function) instead of creating a new lambda for
			each one!]]

***************************************************************************************************
****************************************CHAPTER-FIVE***********************************************
***************************************************************************************************

CHP-5-"Loading and Saving Your Data":





















	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	


















	





	
	

	
	




	
	
	
	
	




		
	
