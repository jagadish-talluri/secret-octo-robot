package com.examples

import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.log4j.Logger

object MainExample {

  def main(arg: Array[String]) {

    var logger = Logger.getLogger(this.getClass())

//    if (arg.length < 2) {
//      logger.error("=> wrong parameters number")
//      System.err.println("Usage: MainExample <path-to-files> <output-path>")
//      System.exit(1)
//    }

    val jobName = "MainExample"

    val conf = new SparkConf().setMaster("local[*]").setAppName(jobName)
    val sc = new SparkContext(conf)
    System.setProperty("hadoop.home.dir","D:\\JagadishTalluri\\winutils")

    //val pathToFiles = "D:\\JagadishTalluri\\Input\\test.txt"
    val outputPath = "D:\\JagadishTalluri\\Output\\test"

    logger.info("=> jobName \"" + jobName + "\"")
    //logger.info("=> pathToFiles \"" + pathToFiles + "\"")

    val files = sc.textFile("D:\\JagadishTalluri\\Input\\test.txt")

    // do your work here
    val rowsWithoutSpaces = files.map(_.replaceAll(" ", ","))

    // and save the result
    rowsWithoutSpaces.saveAsTextFile(outputPath)

  }
}
