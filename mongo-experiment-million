http://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/dont_call_collect_on_a_very_large_rdd.html

SET JAVA_HOME=C:\Program Files\Java\jdk1.8.0_45

D:\JagadishTalluri\Softwares\mongodb-win32-i386-3.0.4\bin

>mongoexport --db test --collection emp --out test.json

>mongoimport --db test --collection pichi --type json --file test.json

cmd>type mongo-data.json

cmd>FOR /l %i in (1,1,10) DO ECHO JAGADISH
-- ----
collection = db.million;
result = collection.aggregate( 
            [
                {"$group": { "_id": { ename: "$ename", desg: "$desg" } } }
            ]
        );
printjson(result);
-- ----
cmd>mongoimport --db test --collection million --type json --file new-1-000-000.json
how much time it took to create? 45minutes, how much size it got created? 563MB. 1 million iterations.
how much time it took for import? 7 minutes. how much time for distinct? 10 sec (1 node,win7-32,ducore)
how man records imported?6690000. How many are inside? 
> db.million.count()
6698718
-- -----
2015-07-01T11:59:56.577+0530    connected to: localhost
2015-07-01T11:59:59.570+0530    [........................] test.million 1.4 MB/563.6 MB (0.3%)
2015-07-01T12:06:44.559+0530    [########################] test.million 563.6 MB/563.6 MB (100.0%)
2015-07-01T12:06:44.664+0530    error inserting documents: assertion C:\data\mci\src\src\mongo/util/concurrency/rwlock.h:170
2015-07-01T12:06:44.668+0530    imported 6690000 documents
-- ---- DATA CORRUPTED, BAD RECORD entered somehow
> db.million.distinct('empid')
[ 101, 102, 103, 104, 105, 106, 107, 108, 134217834 ]
> db.million.find({empid:134217834})
{ "_id" : ObjectId("559388ef50a6dd7fe186c5dd"), "empid" : 134217834, "ename" : "rahul", "desg" : "trainer", "salary" : 30950 }
> db.million.find({empid:134217834}).count()
1

$ kafka-topics.bat --create --zookeeper  localhost:2181 --replication-factor 1 --partions 1 --topic anomaly

-- CITI ACT
-- data ingestion

http://hadoop101.blogspot.in/2011/12/ftp-to-hdfs.html
