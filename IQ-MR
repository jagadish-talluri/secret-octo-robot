1. How to debug an MR program? programatically also?

2. How to use distributed cache in MR program?
Ans: 
  for hadoop 2
  http://wpcertification.blogspot.in/2014/07/using-distributedcache-with-mapreduce.html
  for hadoop 2
  https://gist.github.com/geofferyzh
  
3. What is Configuration and JobConf?

4. How to create/delete the folders/dirs in HDFS programatically?

5. What is GenericOptionsParser?

6. How to create multiple outputs from MR program?

7. How to get a custom name for the output file in MR program?

8. How to create MR program without using Tool Interface?

9. Why do we need to use Tool Interface? What are its pros and cons?

10. How can use Tool and ToolRunner instead of GenericOptionsParser?
Ans:
  With Genericoption you can specify the following (-D option)
  $ hadoop jar /home/hduser/WordCount/wordcount.jar WordCount -Dmapred.reduce.tasks=20 input output

11. API doc for hadoop 1.2.1?
Ans:
  http://hadoop.apache.org/docs/r1.2.1/api/org/apache/hadoop/filecache/DistributedCache.html
  
12. Tutorial doc for apache hadoop 1.2.1 from apache?
Ans:
  https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html#DistributedCache
  
13. Why distributed cache API fully depricated?
Ans:
  http://stackoverflow.com/questions/26492964/are-getcachefiles-and-getlocalcachefiles-the-same
  http://stackoverflow.com/questions/21239722/hadoop-distributedcache-is-deprecated-what-is-the-preferred-api
  
14. Distributed Cache example using old API (mapred)?
Ans: google book link, good one to copy
  https://books.google.co.in/books?id=OlMnCgAAQBAJ&pg=PA92&lpg=PA92&dq=distributed+cache+example+in+hadoop+1.2.1&source=bl&ots=7aM7tGswMy&sig=41T_Se8BVjSrLWrsb4YPrAZsQcs&hl=en&sa=X&ved=0CDkQ6AEwBWoVChMIksjPoN6ayQIVlMGOCh2I9g5i#v=onepage&q=distributed%20cache%20example%20in%20hadoop%201.2.1&f=false

15. What are the jars I used to develop and MR program in hadoop 2?
Ans: Shocking to me, they are 4 files of less tham 5 MB size alltogether. Manually downloaded from MVN repository.
  commons-logging-1.2
  hadoop-common-2.7.1
  hadoop-mapreduce-client-core-2.7.1
  hadoop-mapreduce-client-jobclient-2.7.1
  
16. If you are manually compiling MR job using JAVAC, Which one you compile first? Map/Reduce/Job?
Ans: Tough one, it depends.
  if there are no interdependencies like Map is checking for Job and Job is checking for Map. Then we can go with Map, Reduce and Job.
  But if there are interdependencies then compile all of them in the same command.
  $ javac AMap.java AReduce.java AJob.java
  
18. In hadoop2 what JARS are needed to compile an MR program minimum?
Ans:
  hadoop-common-2.7.1, available at ${HADOOP_HOME}/share/hadoop/common
  hadoop-mapreduce-client-core-2.7.1, available at ${HADOOP_HOME}/share/hadoop/mapreduce

19. JAVAC command used to compile MR program with package structure?
Ans: Sample
  $ javac -d . -cp $HADOOP_LIBS/common/hadoop-common-2.4.0.jar:$HADOOP_LIBS/mapreduce/hadoop-mapreduce-client-core-2.4.0.jar RecordValidationMapper.java RecordValidation.java

20. How to create a JAR based on folder?
Ans: Sample
  $ jar -cvf RecordValidation.jar com

21. In MR program executed on hadoop cluster, Where does the debug System.out.println() statements print?
Ans:
  For Hadoop1 : http://localhost:50030/jobtracker.jsp->click on the completed job->click on map or reduce task->click on tasknumber->task logs->stdout logs.
  For Hadoop2 : yarn logs -applicationId application_1383601692319_0008
  
22. How to use CombineFileInputFormat to send many small files to a single input split?
Ans:
  https://svn.apache.org/repos/asf/hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/MultiFileWordCount.java
  Libs Needed:
    For Hadoop2:
      1. commons-logging-1.2
      2. hadoop-common-2.7.1
      3. hadoop-mapreduce-client-core-2.7.1
    For Hadoop1: Yet to Check

23. Where is hadoop-examples.jar in hadoop installation directory?
Ans:
  For hadoop1: $HADOOP-HOME/hadoop-examples.jar
  For hadoop2: $HADOOP_HOME/share/hadoop/mapreduce/hadoop-examples.jar

24. When you do listing in hdfs, in which oder the files are displayed?
Ans: alphabetic order of the file name

25. How to sort the hdfs -ls output chronologically?
Ans: No direct option, use below
  for hadoop1 : $ hadoop fs -ls /mydir | sort  -k 6
  for hadoop2 : $ hdfs fs -ls /mydir | sort -k 6
  
26. What will you do when you cluster says java-out-of-memory error? You are unable to execute hdfs -ls command also?
Ans:

27. Gotcha, SimpleDateFormat, It is parsing incorrect datetime instead of giving error?
Ans: By default, SimpleDateFormat is liberal or lenient. That means, it is fine if there are mistakes.
  For example: give -> '20150603 25:00:00' , its clear that 25 is bigger than 24, which is incorrect
                but it changes it to '20150604 01:00:00', next day 1st hour.
                so it assumes full liberty. be care full.
  To avoid: explicitly set the lenient to false like, setLenient(false)
  
28. When do we need to have a custom combiner instead of reusing reducer? Any examples?
Ans: Scene 1:
      a. when you want to do a valid/invalid duplicate records segregation, reducer focus on summation logic plus write to 2 files.
      b. And combiner focus on only summation logic(>1 record is duplicate).
     Scene 2:
      a. when you want to do an average in reducer, (sum.of.obs)/(no.of.obs).
      b. you can use combiner to sum up the observation. so this helps in numerator aggregation.

29. What is the parameter to change the input split size?
Ans:    "mapred.max.split.size", this parameter can be set for job individually.

30. Is "dfs.block.size" parameter is truely global?
Ans: Tricky Question.
  According to documentation not necessarily. You can specify some files to have a different block size.
  
31. Assume 64 MB is the size of blocks in HDFS and there are x number of files with that specification in the cluster.
    And Suddenly we changed the block size to 128 MB. So what happens now in this scenario? Does all the old files will also change their block size?
        or Old files retain 64 MB and new files maintain 128 MB?
Ans: Need to check

32. What is the formula used by hadoop to choose the input split?
Ans: " max(minimumSize, min(maximumSize, blockSize)) "
get the MIN of max-split-size=default java long max value AND block-size=default 64/128MB
then get the MAX of previous value AND min-split-size=default is very little.
  by DEFAULT minimumSize < blockSize < maximumSize, THEREFORE mostly blocksize WILL BE split size.
  example:
    Minimum Split Size 1
    Maximum Split Size 32mb
    Block Size  64mb
            Therfore, Split Size  32mb
    explanation: min(64,32) => 32 ; max(-ve, 32) => 32
    
33. What happens if AppMaster Fails? Will there be an re-attempt? If so who will take care of it?
Ans: 

34. What is the need of REST API in mapreduce?
Ans: 

35. Do I need to give the fully qualified path for name node if I want to add files to distributed cache?
Ans: No Need. As input-file and output-file args doesn't need the scheme info. The same cache file also doesnt need it.
    DistributedCache.addCacheFile(new URI(args[0]), job.getConfiguration());
  If at all you add then also no issue, but its like hard coding.
    final String NAME_NODE = "hdfs://somehost:9000";
    DistributedCache.addCacheFile(new URI(NAME_NODE + new Path(args[0])), job.getConfiguration());

36. What is hadoop Stringifier Class?
Ans: It is in Hadooop definitive guide 4th edition page 276. Used to convert objects into string. This is hadoop's own serilalization method.

37. Is HDFS file system can be mounted? if so how?
Ans: Yes, with hadoop2 HDFS file system became mountable.
  Need to know the steps to do:
  
38. 
