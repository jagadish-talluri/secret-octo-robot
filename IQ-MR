1. How to debug an MR program? programatically also?

2. How to use distributed cache in MR program?
Ans: 
  for hadoop 2
  http://wpcertification.blogspot.in/2014/07/using-distributedcache-with-mapreduce.html
  for hadoop 2
  https://gist.github.com/geofferyzh
  
3. What is Configuration and JobConf?

4. How to create/delete the folders/dirs in HDFS programatically?

5. What is GenericOptionsParser?

6. How to create multiple outputs from MR program?

7. How to get a custom name for the output file in MR program?

8. How to create MR program without using Tool Interface?

9. Why do we need to use Tool Interface? What are its pros and cons?

10. How can use Tool and ToolRunner instead of GenericOptionsParser?
Ans:
  With Genericoption you can specify the following (-D option)
  $ hadoop jar /home/hduser/WordCount/wordcount.jar WordCount -Dmapred.reduce.tasks=20 input output

11. API doc for hadoop 1.2.1?
Ans:
  http://hadoop.apache.org/docs/r1.2.1/api/org/apache/hadoop/filecache/DistributedCache.html
  
12. Tutorial doc for apache hadoop 1.2.1 from apache?
Ans:
  https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html#DistributedCache
  
13. Why distributed cache API fully depricated?
Ans:
  http://stackoverflow.com/questions/26492964/are-getcachefiles-and-getlocalcachefiles-the-same
  http://stackoverflow.com/questions/21239722/hadoop-distributedcache-is-deprecated-what-is-the-preferred-api
  
14. Distributed Cache example using old API (mapred)?
Ans: google book link, good one to copy
  https://books.google.co.in/books?id=OlMnCgAAQBAJ&pg=PA92&lpg=PA92&dq=distributed+cache+example+in+hadoop+1.2.1&source=bl&ots=7aM7tGswMy&sig=41T_Se8BVjSrLWrsb4YPrAZsQcs&hl=en&sa=X&ved=0CDkQ6AEwBWoVChMIksjPoN6ayQIVlMGOCh2I9g5i#v=onepage&q=distributed%20cache%20example%20in%20hadoop%201.2.1&f=false

15. What are the jars I used to develop and MR program in hadoop 2?
Ans: Shocking to me, they are 4 files of less tham 5 MB size alltogether. Manually downloaded from MVN repository.
  commons-logging-1.2
  hadoop-common-2.7.1
  hadoop-mapreduce-client-core-2.7.1
  hadoop-mapreduce-client-jobclient-2.7.1
  
16. If you are manually compiling MR job using JAVAC, Which one you compile first? Map/Reduce/Job?
Ans: Tough one, it depends.
  if there are no interdependencies like Map is checking for Job and Job is checking for Map. Then we can go with Map, Reduce and Job.
  But if there are interdependencies then compile all of them in the same command.
  $ javac AMap.java AReduce.java AJob.java
  
18. In hadoop2 what JARS are needed to compile an MR program minimum?
Ans:
  hadoop-common-2.7.1, available at ${HADOOP_HOME}/share/hadoop/common
  hadoop-mapreduce-client-core-2.7.1, available at ${HADOOP_HOME}/share/hadoop/mapreduce

19. JAVAC command used to compile MR program with package structure?
Ans: Sample
  $ javac -d . -cp $HADOOP_LIBS/common/hadoop-common-2.4.0.jar:$HADOOP_LIBS/mapreduce/hadoop-mapreduce-client-core-2.4.0.jar RecordValidationMapper.java RecordValidation.java

20. How to create a JAR based on folder?
Ans: Sample
  $ jar -cvf RecordValidation.jar com

21. In MR program executed on hadoop cluster, Where does the debug System.out.println() statements print?
Ans:
  For Hadoop1 : http://localhost:50030/jobtracker.jsp->click on the completed job->click on map or reduce task->click on tasknumber->task logs->stdout logs.
  For Hadoop2 : yarn logs -applicationId application_1383601692319_0008
  
22. How to use CombineFileInputFormat to send many small files to a single input split?
Ans:
  https://svn.apache.org/repos/asf/hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/MultiFileWordCount.java
  Libs Needed:
    For Hadoop2:
      1. commons-logging-1.2
      2. hadoop-common-2.7.1
      3. hadoop-mapreduce-client-core-2.7.1
    For Hadoop1: Yet to Check

23. Where is hadoop-examples.jar in hadoop installation directory?
Ans:
  For hadoop1: $HADOOP-HOME/hadoop-examples.jar
  For hadoop2: $HADOOP_HOME/share/hadoop/mapreduce/hadoop-examples.jar

24. When you do listing in hdfs, in which oder the files are displayed?
Ans: alphabetic order of the file name

25. How to sort the hdfs -ls output chronologically?
Ans: No direct option, use below
  for hadoop1 : $ hadoop fs -ls /mydir | sort  -k 6
  for hadoop2 : $ hdfs fs -ls /mydir | sort -k 6
  
26. What will you do when you cluster says java-out-of-memory error? You are unable to execute hdfs -ls command also?
Ans:

27. Gotcha, SimpleDateFormat, It is parsing incorrect datetime instead of giving error?
Ans: By default, SimpleDateFormat is liberal or lenient. That means, it is fine if there are mistakes.
  For example: give -> '20150603 25:00:00' , its clear that 25 is bigger than 24, which is incorrect
                but it changes it to '20150604 01:00:00', next day 1st hour.
                so it assumes full liberty. be care full.
  To avoid: explicitly set the lenient to false like, setLenient(false)
  
28. When do we need to have a custom combiner instead of reusing reducer? Any examples?
Ans: Scene 1:
      a. when you want to do a valid/invalid duplicate records segregation, reducer focus on summation logic plus write to 2 files.
      b. And combiner focus on only summation logic(>1 record is duplicate).
     Scene 2:
      a. when you want to do an average in reducer, (sum.of.obs)/(no.of.obs).
      b. you can use combiner to sum up the observation. so this helps in numerator aggregation.

29. What is the parameter to change the input split size?
Ans:    "mapred.max.split.size", this parameter can be set for job individually.

30. Is "dfs.block.size" parameter is truely global?
Ans: Tricky Question.
  According to documentation not necessarily. You can specify some files to have a different block size.
  
31. Assume 64 MB is the size of blocks in HDFS and there are x number of files with that specification in the cluster.
    And Suddenly we changed the block size to 128 MB. So what happens now in this scenario? Does all the old files will also change their block size?
        or Old files retain 64 MB and new files maintain 128 MB?
Ans: Need to check

32. What is the formula used by hadoop to choose the input split?
Ans: " max(minimumSize, min(maximumSize, blockSize)) "
get the MIN of max-split-size=default java long max value AND block-size=default 64/128MB
then get the MAX of previous value AND min-split-size=default is very little.
  by DEFAULT minimumSize < blockSize < maximumSize, THEREFORE mostly blocksize WILL BE split size.
  example:
    Minimum Split Size 1
    Maximum Split Size 32mb
    Block Size  64mb
            Therfore, Split Size  32mb
    explanation: min(64,32) => 32 ; max(-ve, 32) => 32
    
33. What happens if AppMaster Fails? Will there be an re-attempt? If so who will take care of it?
Ans: 

34. What is the need of REST API in mapreduce?
Ans: 

35. Do I need to give the fully qualified path for name node if I want to add files to distributed cache?
Ans: No Need. As input-file and output-file args doesn't need the scheme info. The same cache file also doesnt need it.
    DistributedCache.addCacheFile(new URI(args[0]), job.getConfiguration());
  If at all you add then also no issue, but its like hard coding.
    final String NAME_NODE = "hdfs://somehost:9000";
    DistributedCache.addCacheFile(new URI(NAME_NODE + new Path(args[0])), job.getConfiguration());

36. What is hadoop Stringifier Class?
Ans: It is in Hadooop definitive guide 4th edition page 276. Used to convert objects into string. This is hadoop's own serilalization method.

37. Is HDFS file system can be mounted? if so how?
Ans: Yes, with hadoop2 HDFS file system became mountable.
  Need to know the steps to do:
  
38. Where do you see the logger messages or sysout's or exception messages emitted by your MR job?
Ans: Lot to write, working on it.

39. How many blocks are created for a file size of 200 MB? (block-size=64MB, replication=1)
Ans: 4 blocks
    1 block 64MB of 1-64(64MB)
    2 block 64MB of 65-128(64MB)
    3 block 64MB of 129-192(64MB)
    4 block 64MB of 193-200(8MB)
    
40. In above question(39). If the last block is not completely filled then what happens? Is the remaining spaces wasted? Explain?
Ans: No. Remaining 56MB is not wasted and can be usable if needed. HDFS is a logical file system not physical. So we are not supposed
      to take these numbers seriously like a real file system.
    Explanation: In progress
        How the unused space accounted for?
        http://stackoverflow.com/questions/13012924/large-block-size-in-hdfs-how-is-the-unused-space-accounted-for
      
41. Are the blocks of a file are written sequentially or parallely? Look at 39 Question for the file.
Ans: Sequential, one after other. Refer the below link.
  http://stackoverflow.com/questions/24301085/hadoop-hdfs-file-writes-reads

42. How will you read a binary file such as image, video or some other file has binary data?
Ans: we use BytesWritable data type to read it. There is an example of video analysis.
      see in youtube

43. What is short circut read in Hadoop/HDFS?
Ans: In HDFS, reads normally go through the DataNode. Thus, when the client asks the DataNode to read a file, 
  the DataNode reads that file off of the disk and sends the data to the client over a TCP socket. 
  So-called “short-circuit” reads bypass the DataNode, allowing the client to read the file directly. 
  Obviously, this is only possible in cases where the client is co-located with the data. 
  Short-circuit reads provide a substantial performance boost to many applications.
  setting is done in hdfs-site.xml.

44. Why do you get "Unable to load native-hadoop library for your platform... using built-in-java classes where applicable" this error?
Ans: If hadoop didnt find the *.so files then it will give this error.
   Depending on your environment, the term “native libraries” could refer to all *.so’s you need to compile; 
   and, the term “native compression” could refer to all *.so’s you need to compile that are specifically related to compression. 
   Currently, however, this document only addresses the native hadoop library (libhadoop.so). The document for libhdfs library (libhdfs.so)

45. Why do you need native libraries? What special usage than Java? Why the .co extension?
Ans: 
  Hadoop has native implementations of certain components for performance reasons and for non-availability of Java implementations.
  The native hadoop library is written in ANSI C and is built using the GNU autotools-chain (autoconf, autoheader, automake, autoscan, libtool). 
  This means it should be straight-forward to build the library on any platform with a standards-compliant C compiler and the GNU autotools-chain (see the supported platforms).
